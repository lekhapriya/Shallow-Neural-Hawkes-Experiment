{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesKernelPowerLaw, SimuHawkes, HawkesKernelTimeFunc\n",
    "from tick.base import TimeFunction\n",
    "optimalParams = []\n",
    "nNeurons = 100\n",
    "\n",
    "Alphas = np.zeros([nNeurons,4])\n",
    "Alpha0 = -np.zeros([1,4])\n",
    "Betas = np.zeros([nNeurons,4])\n",
    "Beta0 = np.zeros([nNeurons,4])\n",
    "mu = np.random.uniform(0,1,2)\n",
    "\n",
    "epsilon = 1e-8\n",
    "\n",
    "Dict={x: {} for x in range(4)}\n",
    "Dict_integrate = {x: {} for x in range(4)}  #nested dictionary for all networks\n",
    "Dict_gradient = {x: {} for x in range(4)}\n",
    "\n",
    "MapT1T2={}\n",
    "adjacentKernel={0:[0,1],1:[3,2]}\n",
    "para = np.zeros([4,2])\n",
    "#power kernel = alpha*(delta+t)^(-beta)\n",
    "\n",
    "#simulation part\n",
    "\n",
    "\n",
    "\n",
    "def createMapAtoBIndex(a,b):\n",
    "    mapAtoBIndex={}\n",
    "    for x in a:\n",
    "        if(max(b[b<x],default=-1)==-1):\n",
    "            mapAtoBIndex[x] = None\n",
    "        else:\n",
    "            mapAtoBIndex[x] = (np.where(b==max(b[b<x])))\n",
    "    return mapAtoBIndex\n",
    "    \n",
    "\n",
    "\n",
    "def exponentialIntegratedKernel(tend,kernel):\n",
    "    alpha = para[kernel,0]\n",
    "    beta = para[kernel,1]\n",
    "    \n",
    "    res = (alpha/beta)*(1-np.exp(-beta*tend))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def exponentialKernel(tp,kernel):\n",
    "    alpha = para[kernel,0]\n",
    "    beta = para[kernel,1]\n",
    "    res = alpha*np.exp(-beta*tp)\n",
    "   \n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def loglikelihood():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(exponentialIntegratedKernel(tend.reshape(1,-1),p))\n",
    "        \n",
    "        a = a + np.sum(exponentialIntegratedKernel(tend.reshape(1,-1),p+2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ll = ll+ mu[p]*T+a\n",
    "       \n",
    "        \n",
    "        ll = ll-np.log(mu[p])\n",
    "       \n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-30,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(exponentialKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0])\n",
    "                lj = max(j-30,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(exponentialKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            \n",
    "            ll = ll+logLam\n",
    "        \n",
    "    return ll\n",
    "\n",
    "def gradientIntegratedKernel(tend,p):\n",
    "    alpha = para[p,0]\n",
    "    beta = para[p,1]\n",
    "    fac1 = np.exp(-beta*tend)\n",
    "    val = np.zeros([2])\n",
    "    val[0]=(1/beta)*(1-fac1)\n",
    "   \n",
    "    fac2 = (alpha/beta)*(-(1/beta)+fac1*((1/beta)+tend))\n",
    "    val[1] = fac2\n",
    "    return val\n",
    "\n",
    "def gradientExpKernel(temp,p):\n",
    "    alpha = para[p,0]\n",
    "    beta = para[p,1]\n",
    "    fac1 = np.exp(-beta*temp)\n",
    "    val = np.zeros([2])\n",
    "    val[0]=np.sum(fac1)\n",
    "    val[1]= -alpha*np.sum(temp*fac1)\n",
    "    return val\n",
    "    \n",
    "\n",
    "def gradientll(iArray):\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    \n",
    "    grad = np.zeros([4,2])\n",
    "    for i in range(0,max(iArray.shape),1):\n",
    "        factor1=0\n",
    "        factor2=0\n",
    "        p = iArray[0,i]\n",
    "        index = iArray[1,i]\n",
    "        tend = (T-t[p][index])\n",
    "        grad[p,:]=grad[p,:]+gradientIntegratedKernel(tend,p)\n",
    "        \n",
    "        grad[p+2,:]=grad[p+2,:]+gradientIntegratedKernel(tend,p+2)\n",
    "        \n",
    "        li = max(index-30,0)\n",
    "        temp1 = t[p][index]-t[p][li:index]\n",
    "        decayFactor = np.sum(exponentialKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "        factor1=  gradientExpKernel(temp1,adjacentKernel[p][0])\n",
    "        jT = MapT1T2[p][t[p][index]]\n",
    "        otherP = (p==0)*1\n",
    "        if(jT != None):\n",
    "            j = np.asscalar(jT[0])\n",
    "            lj = max(j-30,0)\n",
    "            temp2 = t[p][index]-t[otherP][lj:j+1]\n",
    "            decayFactor = decayFactor + np.sum(exponentialKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            factor2 = gradientExpKernel(temp2,adjacentKernel[p][1])\n",
    "        \n",
    "        lam = mu[p]+decayFactor\n",
    "        grad[adjacentKernel[p][0],:] = grad[adjacentKernel[p][0],:] - (1/lam)*factor1\n",
    "        \n",
    "        grad[adjacentKernel[p][1],:] = grad[adjacentKernel[p][1],:] - (1/lam)*factor2\n",
    "        \n",
    "    print(grad/max(iArray.shape))\n",
    "                             \n",
    "    return grad/max(iArray.shape)\n",
    "\n",
    "def sgdHawkes():\n",
    "    global para\n",
    "    lr =0.01\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "    epsilon = 1e-8\n",
    "    count = 0\n",
    "    bestll = 1e8\n",
    "    bestpara = np.zeros([4,2])\n",
    "    m_t = np.zeros([4,2])\n",
    "    v_t = np.zeros([4,2])\n",
    "    lenT1 = len(t[0][:])\n",
    "    lenT2 = len(t[1][:])\n",
    "    totalLength = lenT1+lenT2\n",
    "    tCompressed = np.zeros((2,totalLength)).astype(int)\n",
    "    tCompressed[:,lenT1:totalLength]=1\n",
    "    tCompressed[1,0:lenT1]=range(0,lenT1,1)\n",
    "    tCompressed[1,lenT1:totalLength]=range(0,lenT2,1)\n",
    "    \n",
    "    for epochs in range(1,30,1):\n",
    "        \n",
    "        rsample = np.random.choice(totalLength,totalLength,replace = False)\n",
    "        for i in range(0,len(rsample),100):\n",
    "            count=count+1 \n",
    "            grad = gradientll(tCompressed[:,rsample[i:i+100]])\n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad*grad)\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            para = para-(lr*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            \n",
    "            error=loglikelihood()\n",
    "            error2=alternate_loglikelihood()\n",
    "            bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            para = np.maximum(para,0)\n",
    "            bestll = min(bestll,error)\n",
    "            \n",
    "            print(i,error,error2, bestll,'\\n',para)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara  \n",
    "    \n",
    "\n",
    "## plots\n",
    "def plotKernels(p):\n",
    "    showIntegratedKernel = True\n",
    "    dx=0.01\n",
    "    \n",
    "    tk = np.arange(0,5,dx)\n",
    "    interestX = Dict[p]['inflection']\n",
    "    \n",
    "    y1 = alpha*np.exp(-beta*tk).reshape(-1)\n",
    "    y2 = nnKernel(tk.reshape(1,-1),p).reshape(-1)\n",
    "    y3 = (alpha/beta)*(1 - np.exp(-beta*tk)).reshape(-1)\n",
    "    y4= np.cumsum(y2*dx).reshape(-1)\n",
    "    y5 = nnIntegratedKernel(tk.reshape(1,-1),p).reshape(-1)\n",
    "    \n",
    "    plt.plot(tk.reshape(-1), y1,'tab:blue')\n",
    "    plt.plot(tk.reshape(-1),y2,'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        plt.plot(tk.reshape(-1), y3,'tab:green')\n",
    "        plt.plot(tk.reshape(-1),y4,'tab:red')\n",
    "        plt.plot(tk.reshape(-1),y5,'tab:brown')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.pause(0.005)\n",
    "\n",
    "def plotKernelsAll():\n",
    "    \n",
    "    showIntegratedKernel = False\n",
    "    dx=0.01\n",
    "    \n",
    "    tk = np.arange(0,5,dx)\n",
    "    y2 = np.zeros([len(tk),4])\n",
    "    y4 = np.zeros([len(tk),4])\n",
    "    y5 = np.zeros([len(tk),4])\n",
    "    \n",
    "    for p in range(4):\n",
    "        interestX = Dict[p]['inflection']\n",
    "    \n",
    "        y2[:,p] = nnKernel(tk.reshape(1,-1),p)\n",
    "\n",
    "        y4[:,p] = np.cumsum(y2[:,p]*dx)\n",
    "        y5[:,p] = nnIntegratedKernel(tk.reshape(1,-1),p)\n",
    "\n",
    "\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    p=0\n",
    "    fig.suptitle('Sharing x per column, y per row')\n",
    "    ax1.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax1.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax1.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p=1\n",
    "    \n",
    "    ax2.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax2.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax2.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p= 2\n",
    "    \n",
    "    ax3.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax3.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax3.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p=3\n",
    "    ax4.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax4.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax4.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "   \n",
    "    return\n",
    "## Code for Neural Hawkes starts here##\n",
    "\n",
    "def inflectionPoints():\n",
    "    \n",
    "    for p in range(4):\n",
    "        alphas = Alphas[:,p]\n",
    "    \n",
    "        betas = Betas[:,p]\n",
    "        beta0 = Beta0[:,p]\n",
    "    \n",
    "        div = betas+epsilon*(np.abs(betas)<epsilon) # potentially error prone\n",
    "        x = -beta0/div\n",
    "        interestX1 = x*(x>0)\n",
    "        alwaysInclude = (x<=0)*(betas>0) #dont change\n",
    "        alwaysExclude = (x<=0)*(betas<0)\n",
    "        tempX = x*(~alwaysInclude)*(~alwaysExclude)\n",
    "        interestX1 = tempX[interestX1>0]\n",
    "        interestX = np.sort(interestX1)\n",
    "        interestX = np.append(0,interestX)\n",
    "        \n",
    "        Dict[p]['inflection'] = interestX\n",
    "   \n",
    "        con = alphas*betas\n",
    "        Dict[p]['constant'] = con\n",
    "    return\n",
    "\n",
    "def nnIntegratedKernel(x,p):\n",
    "    x = x.reshape(-1)\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p].reshape(-1,1)\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    const1 = Dict[p]['constant']\n",
    "    interestX = Dict[p]['inflection']\n",
    "    precalculate_integrate(p)\n",
    "    \n",
    "    y = np.zeros([1,max(x.shape)])\n",
    "    \n",
    "    for i in range(0,max(x.shape)):\n",
    "        xi = x[i]\n",
    "        if(xi>0):\n",
    "            iP = max(interestX[interestX<xi])\n",
    "            n1 = betas*(xi-epsilon) + beta0\n",
    "            dn1 = (n1>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "            term1 = nnKernel(xi,p)*((const!=0)+xi*(const==0))\n",
    "            term2 = nnKernel(iP,p)*((const!=0)+iP*(const==0))\n",
    "            \n",
    "            const = (const)*(const!=0)+(const==0)*1.0\n",
    "            \n",
    "            prev_term = Dict_integrate[p][iP]\n",
    "            \n",
    "            y[0,i] = prev_term + ((term1-term2)/(const))\n",
    "        \n",
    "       \n",
    "               \n",
    "\n",
    "    return y\n",
    "\n",
    "def nnKernel(x,p):\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p]\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y    \n",
    "\n",
    "\n",
    "def initializeParams(nNeurons):\n",
    "    \n",
    "    \n",
    "    for kernel in range(4):\n",
    "        Alphas[:,kernel] = -(np.random.uniform(0,1,nNeurons)).reshape(-1)*0.5\n",
    "        Alpha0[:,kernel] = -np.random.uniform(0,1,1)*0.5\n",
    "        Betas[:,kernel] = (np.random.uniform(0,1,nNeurons)).reshape(-1)*0.5\n",
    "        Beta0[:,kernel] = np.random.uniform(0,1,nNeurons).reshape(-1)*0.05\n",
    "       \n",
    "    return mu\n",
    "\n",
    "\n",
    "\n",
    "def precalculate_integrate(kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    iP = Dict[kernel]['inflection']\n",
    "    const1 = Dict[kernel]['constant']\n",
    "    Dict_integrate[kernel].clear()\n",
    "    Dict_integrate[kernel][0]=0\n",
    "    y = 0\n",
    "    for index in range(1,len(iP)):\n",
    "        n1 = betas*(iP[index]-epsilon) + beta0\n",
    "        dn1 = (n1>0)\n",
    "        const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "        term1 = nnKernel(iP[index],kernel)*((const!=0)+iP[index]*(const==0))\n",
    "        term2 = nnKernel(iP[index-1],kernel)*((const!=0)+iP[index-1]*(const==0))\n",
    "            \n",
    "        const = (const)*(const!=0)+(const==0)*1.0\n",
    "              \n",
    "        y= y + ((term1-term2)/(const))\n",
    "        Dict_integrate[kernel][iP[index]]=y\n",
    "    return\n",
    "    \n",
    "def precalculate_gradient(kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    iP = Dict[kernel]['inflection']\n",
    "    \n",
    "    const1 = Dict[kernel]['constant']\n",
    "    Dict_gradient[kernel].clear()\n",
    "    Dict_gradient[kernel][0]=list([gradA0,gradA,gradB1,gradB0])\n",
    "    for index in range(1,len(iP)):\n",
    "            \n",
    "            n0pr = betas*(iP[index-1]+epsilon)+beta0\n",
    "            n1pr = betas*(iP[index]-epsilon) + beta0\n",
    "            dn1 = (n1pr>0)\n",
    "            dn0 = (n0pr>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            indicator = const==0\n",
    "            const = const*(const!=0)+1*(const==0)\n",
    "            n0 = betas*(iP[index-1])+beta0\n",
    "            n1 = betas*(iP[index]) + beta0\n",
    "            \n",
    "            fac1 = nnKernel(iP[index],kernel)\n",
    "            fac2 = nnKernel(iP[index-1],kernel)\n",
    "            gradA0 = gradA0 + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(iP[index]-iP[index-1])\n",
    "            gradA = gradA -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            gradB1 = gradB1 -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradB1 = gradB1 + ((1/const)*(alphas)*(fac1*iP[index]*dn1-fac2*iP[index-1]*dn0))*(~indicator)\n",
    "            gradB0 = gradB0+ ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "            Dict_gradient[kernel][iP[index]]= list([gradA0,gradA,gradB1,gradB0])\n",
    "            \n",
    "    return\n",
    "    \n",
    "def gradientNNIntegratedKernel(tend,kernel):\n",
    "    \n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    \n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    \n",
    "    if(tend>0):\n",
    "        const1 = Dict[kernel]['constant']\n",
    "        interestX = Dict[kernel]['inflection']\n",
    "            \n",
    "        iP = max(interestX[interestX<tend])\n",
    "        n1pr = betas*(tend-epsilon) + beta0\n",
    "        n0pr = betas*(tend+epsilon)+beta0\n",
    "           \n",
    "        dn1 = (n1pr>0)\n",
    "        dn0 = (n0pr>0)\n",
    "        const =np.dot(const1.T,dn1)\n",
    "        indicator = const==0\n",
    "        const = const*(const!=0)+1*(const==0)\n",
    "        n0 = betas*(iP)+beta0\n",
    "        n1 = betas*(tend) + beta0\n",
    "        fac1 = nnKernel(tend,kernel)\n",
    "        fac2 = nnKernel(iP,kernel)\n",
    "        gradients = Dict_gradient[kernel][iP]\n",
    "            \n",
    "            \n",
    "        gradA0 = gradA0+gradients[0] + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(tend-iP)\n",
    "            \n",
    "        gradA = gradA + gradients[1] -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "        gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            \n",
    "        gradB1 = gradB1+gradients[2] -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "        gradB1 = gradB1+ ((1/const)*(alphas)*(fac1*tend*dn1-fac2*iP*dn0))*(~indicator)\n",
    "    \n",
    "        gradB0 =gradB0+ gradients[3] + ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "    return list([gradA0,gradA,gradB1,gradB0])\n",
    "\n",
    "def gradientNNKernel(temp,kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel]\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    \n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    fac1 = nnKernel(temp.reshape(1,-1),kernel)\n",
    "    \n",
    "    \n",
    "   \n",
    "    n1 = np.dot(betas,temp.reshape(1,-1)) + beta0\n",
    "    gradA = gradA + np.sum(fac1*np.maximum(n1,0),axis=1).reshape(-1,1)\n",
    "        \n",
    "    gradA0 = gradA0 + np.sum(fac1)\n",
    "        \n",
    "    gradB1 = gradB1 + np.sum(fac1*(n1>0)*alphas*temp.reshape(1,-1),axis=1).reshape(-1,1)\n",
    "    gradB0 = gradB0 + np.sum(fac1*(n1>0)*alphas,axis=1).reshape(-1,1)\n",
    "    return list([gradA0,gradA,gradB1,gradB0])\n",
    "    \n",
    "    \n",
    "\n",
    "def gradientNetwork(iArray):\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    gradA0 = np.zeros([1,4])\n",
    "    gradA = np.zeros([nNeurons,4])\n",
    "    gradB1 = np.zeros([nNeurons,4])\n",
    "    gradB0 = np.zeros([nNeurons,4])\n",
    "    grad_mu = np.zeros(2)\n",
    "    nSamples = max(iArray.shape)\n",
    "    \n",
    "    for kernel in range(0,4,1):\n",
    "        precalculate_gradient(kernel)\n",
    "    \n",
    "    for i in range(0,nSamples,1):\n",
    "        p = iArray[0,i]\n",
    "        index = iArray[1,i]\n",
    "        tend = (T-t[p][index])\n",
    "        out = gradientNNIntegratedKernel(tend,p)\n",
    "       \n",
    "        gradA0[:,p]=gradA0[:,p]+out[0]\n",
    "        gradA[:,p]=gradA[:,p]+out[1].reshape(-1)\n",
    "        gradB1[:,p]=gradB1[:,p]+out[2].reshape(-1)\n",
    "        gradB0[:,p]=gradB0[:,p]+out[3].reshape(-1)\n",
    "        \n",
    "        out = gradientNNIntegratedKernel(tend,p+2)\n",
    "        gradA0[:,p+2]=gradA0[:,p+2]+out[0]\n",
    "        gradA[:,p+2]=gradA[:,p+2]+out[1].reshape(-1)\n",
    "        gradB1[:,p+2]=gradB1[:,p+2]+out[2].reshape(-1)\n",
    "        gradB0[:,p+2]=gradB0[:,p+2]+out[3].reshape(-1)\n",
    "        \n",
    "        li = max(index-40,0)\n",
    "        kernel0 = adjacentKernel[p][0]\n",
    "        kernel1 = adjacentKernel[p][1]\n",
    "        temp1 = t[p][index]-t[p][li:index]\n",
    "        decayFactor = np.sum(nnKernel(temp1.reshape(1,-1),kernel0))\n",
    "        out =  gradientNNKernel(temp1,kernel0)\n",
    "        jT = MapT1T2[p][t[p][index]]\n",
    "        otherP = (p==0)*1\n",
    "        if(jT != None):\n",
    "            j = np.asscalar(jT[0][0])\n",
    "            lj = max(j-40,0)\n",
    "            temp2 = t[p][index]-t[otherP][lj:j+1]\n",
    "            decayFactor = decayFactor + np.sum(nnKernel(temp2.reshape(1,-1),kernel1))\n",
    "            out2 = gradientNNKernel(temp2,kernel1)\n",
    "        \n",
    "        lam = mu[p]+decayFactor\n",
    "        invLam = (1/lam)\n",
    "        \n",
    "        \n",
    "        gradA0[:,kernel0]=gradA0[:,kernel0] -invLam*out[0]\n",
    "        gradA[:,kernel0]=gradA[:,kernel0] -invLam*out[1].reshape(-1)\n",
    "        gradB1[:,kernel0]=gradB1[:,kernel0]-invLam*out[2].reshape(-1)\n",
    "        gradB0[:,kernel0]=gradB0[:,kernel0] -invLam*out[3].reshape(-1)\n",
    "        \n",
    "        if(jT != None):\n",
    "            gradA0[:,kernel1]=gradA0[:,kernel1] -invLam*out2[0]\n",
    "            gradA[:,kernel1]=gradA[:,kernel1] -invLam*out2[1].reshape(-1)\n",
    "            gradB1[:,kernel1]=gradB1[:,kernel1]-invLam*out2[2].reshape(-1)\n",
    "            gradB0[:,kernel1]=gradB0[:,kernel1] -invLam*out2[3].reshape(-1)\n",
    "        grad_mu[p]=grad_mu[p]+((t[p][index]-t[p][index-1])-(1/lam))*(index >0) \n",
    "        \n",
    "    gradA0 = gradA0/(nSamples)\n",
    "    gradA = gradA/nSamples\n",
    "    gradB1 = gradB1/nSamples\n",
    "    gradB0 = gradB0/nSamples\n",
    "        \n",
    "    return list([gradA0,gradA,gradB1,gradB0,grad_mu])\n",
    "     \n",
    "\n",
    "\n",
    "def nnLoglikelihood():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(nnIntegratedKernel(tend.reshape(1,-1),p))\n",
    "       \n",
    "        \n",
    "        a = a + np.sum(nnIntegratedKernel(tend.reshape(1,-1),p+2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ll = ll+ mu[p]*T+a\n",
    "       \n",
    "        \n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(nnKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(nnKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            \n",
    "            ll = ll+logLam\n",
    "       \n",
    "    return ll\n",
    "\n",
    "def sgdNeuralHawkesBiVariate(nEpochs,lr):\n",
    "    \n",
    "    global Alpha0\n",
    "    global Alphas\n",
    "    global Betas\n",
    "    global Beta0\n",
    "    global mu\n",
    "    \n",
    "    lr2 =lr*0.1\n",
    "    lr_mu = lr*0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "  \n",
    "    \n",
    "    bestll = 1e8\n",
    "    neg_ll = []\n",
    "    \n",
    "    optimalParams = list([Alpha0,Alphas,Betas,Beta0])\n",
    "    \n",
    "    m_t_A = np.zeros([nNeurons,4])\n",
    "    m_t_A0 =np.zeros([1,4])\n",
    "    m_t_B= np.zeros([nNeurons,4])\n",
    "    m_t_B0= np.zeros([nNeurons,4])\n",
    "    m_t = 0\n",
    "    v_t_A = np.zeros([nNeurons,4])\n",
    "    v_t_A0 =np.zeros([1,4])\n",
    "    v_t_B= np.zeros([nNeurons,4])\n",
    "    v_t_B0= np.zeros([nNeurons,4])\n",
    "    v_t = 0\n",
    "    count = 0\n",
    "    lenT1 = len(t[0][:])\n",
    "    lenT2 = len(t[1][:])\n",
    "    totalLength = lenT1+lenT2\n",
    "    tCompressed = np.zeros((2,totalLength)).astype(int)\n",
    "    tCompressed[:,lenT1:totalLength]=1\n",
    "    tCompressed[1,0:lenT1]=range(0,lenT1,1)\n",
    "    tCompressed[1,lenT1:totalLength]=range(0,lenT2,1)\n",
    "    \n",
    "    for epochs in range(1,30,1):\n",
    "        inflectionPoints()\n",
    "        rsample = np.random.choice(totalLength,totalLength,replace = False)\n",
    "        for i in range(0,len(rsample),20):\n",
    "            count=count+1 \n",
    "            grad = gradientNetwork(tCompressed[:,rsample[i:i+20]])\n",
    "            \n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad[4]\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad[4]*grad[4])\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            mu = mu-(lr_mu*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            mu = np.maximum(mu,0)\n",
    "           \n",
    "           \n",
    "            m_t_A0 = beta_1*m_t_A0 + (1-beta_1)*grad[0]\t#updates the moving averages of the gradient\n",
    "            v_t_A0 = beta_2*v_t_A0 + (1-beta_2)*(grad[0]*grad[0])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A0 = m_t_A0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A0 = v_t_A0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Alpha0 = Alpha0-(lr*m_cap_A0)/(np.sqrt(v_cap_A0)+epsilon) \n",
    "                         \n",
    "                   \n",
    "            m_t_A = beta_1*m_t_A + (1-beta_1)*grad[1]\t#updates the moving averages of the gradient\n",
    "            v_t_A = beta_2*v_t_A + (1-beta_2)*(grad[1]*grad[1])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A = m_t_A/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A = v_t_A/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Alphas = Alphas-(lr*m_cap_A)/(np.sqrt(v_cap_A)+epsilon)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "                \n",
    "            m_t_B = beta_1*m_t_B + (1-beta_1)*grad[2]\t#updates the moving averages of the gradient\n",
    "            v_t_B = beta_2*v_t_B + (1-beta_2)*(grad[2]*grad[2])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B = m_t_B/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B= v_t_B/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Betas = Betas-(lr2*m_cap_B)/(np.sqrt(v_cap_B)+epsilon)\n",
    "            \n",
    "            \n",
    "            m_t_B0 = beta_1*m_t_B0 + (1-beta_1)*grad[3]\t#updates the moving averages of the gradient\n",
    "            v_t_B0 = beta_2*v_t_B0 + (1-beta_2)*(grad[3]*grad[3])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B0 = m_t_B0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B0 = v_t_B0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Beta0 = Beta0 -(lr2*m_cap_B0)/(np.sqrt(v_cap_B0)+epsilon)\n",
    "          \n",
    "            inflectionPoints()\n",
    "            \n",
    "        error=nnLoglikelihood()\n",
    "        neg_ll.append(error)\n",
    "            #bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            \n",
    "        if(bestll > error):\n",
    "            optimalParams = list([Alpha0,Alphas,Betas,Beta0,mu])\n",
    "                \n",
    "               \n",
    "            \n",
    "        bestll = min(bestll,error)\n",
    "        print(i,epochs,bestll,error,mu)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara\n",
    "    return optimalParams,neg_ll,nEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate neg ll of snh\n",
    "def nnOptimalKernel(x,p):\n",
    "    optimalkernel = SGD[0]\n",
    "    Alpha0 = optimalkernel[0]\n",
    "    Alphas = optimalkernel[1]\n",
    "    Betas = optimalkernel[2]\n",
    "    Beta0 = optimalkernel[3]\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p]\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y\n",
    "\n",
    "def customIntegrateKernel(tend,p):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = nnOptimalKernel(t3.reshape(1,-1),p).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def SNH_Neg_LL():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernel(tend.reshape(-1),p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernel(tend.reshape(-1),p+2))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(nnOptimalKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(nnOptimalKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "#to calculate neg ll of em\n",
    "otherKernel={0:[1],1:[0]}\n",
    "\n",
    "def customIntegrateKernelEM(tend,m,n):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = em.get_kernel_values(m,n,t3.reshape(1,-1)).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def EM_Neg_ll():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernelEM(tend.reshape(-1),p,p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernelEM(tend.reshape(-1),otherKernel[p][0],p))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(em.get_kernel_values(p,p, temp1.reshape(1,-1)))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(em.get_kernel_values(p,otherKernel[p],temp2.reshape(1,-1)))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "#to calculate Neg_ll of WH\n",
    "\n",
    "def customIntegrateKernelWH(tend,m,n):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = wh.get_kernel_values(m,n,t3.reshape(1,-1)).reshape(-1)\n",
    "    ytemp[ytemp < 0] = 0\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def WH_Neg_ll():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernelWH(tend.reshape(-1),p,p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernelWH(tend.reshape(-1),otherKernel[p][0],p))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            nk = wh.get_kernel_values(p,p, temp1.reshape(1,-1))\n",
    "            nk[nk < 0] = 0\n",
    "            decayFactor = np.sum(nk)\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                nk = wh.get_kernel_values(p,otherKernel[p][0],temp2.reshape(1,-1))\n",
    "                nk[nk < 0] = 0\n",
    "                decayFactor = decayFactor + np.sum(nk)\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 83574\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/lekhapriya/SNH/master/Combined.csv\"\n",
    "dataset = pd.read_csv(url, names=['Timestamp', 'Price','Volume','Buyer ID','Seller ID','Buyer is market maker'])\n",
    "dataset = dataset.sort_values(dataset.columns[0])\n",
    "dataset[\"Timestamp\"]=(dataset[\"Timestamp\"]-min(dataset[\"Timestamp\"]))/1000\n",
    "\n",
    "buy_df = dataset.loc[~dataset.iloc[:,5]]\n",
    "sell_df = dataset.loc[dataset.iloc[:,5]]\n",
    "buy = buy_df.drop_duplicates(['Buyer ID'])\n",
    "sell = sell_df.drop_duplicates(['Seller ID'])\n",
    "\n",
    "#time\n",
    "t_sell = ((sell.iloc[:,0]).values).reshape(-1,1)\n",
    "t_buy = ((buy.iloc[:,0]).values).reshape(-1,1)\n",
    "\n",
    "sell = np.concatenate((t_sell,np.ones_like(t_sell)),axis=1)\n",
    "buy = np.concatenate((t_buy,np.zeros_like(t_buy)),axis=1)\n",
    "\n",
    "total_events = np.concatenate((sell,buy))\n",
    "sorted_time = total_events[np.argsort(total_events[:, 0])]\n",
    "print('N =',len(t_sell)+len(t_buy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 13926 13927 13928] TEST: [13929 13930 13931 ... 27855 27856 27857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:531: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:595: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13920 1 -9681.295005193386 -9681.295005193386 [0.70809495 0.54131132]\n",
      "13920 2 -10006.442055632288 -10006.442055632288 [0.87082425 0.70685722]\n",
      "13920 3 -10206.154809807224 -10206.154809807224 [0.97254259 0.8534019 ]\n",
      "13920 4 -10457.942647645765 -10457.942647645765 [0.99248098 0.97894214]\n",
      "13920 5 -10576.114515740825 -10576.114515740825 [1.02994229 1.05189536]\n",
      "13920 6 -10581.309703724255 -10581.309703724255 [1.06809474 1.12213927]\n",
      "13920 7 -10589.330675281675 -10589.330675281675 [1.10432688 1.16707346]\n",
      "13920 8 -10607.926323296384 -10607.926323296384 [1.13108344 1.19265509]\n",
      "13920 9 -10619.070841752828 -10619.070841752828 [1.15501234 1.20614148]\n",
      "13920 10 -10619.070841752828 -10617.01872699326 [1.16098258 1.22745623]\n",
      "13920 11 -10633.531203611028 -10633.531203611028 [1.16453701 1.24231683]\n",
      "13920 12 -10633.531203611028 -10623.946297315295 [1.16901593 1.25849495]\n",
      "13920 13 -10633.531203611028 -10617.615662528693 [1.17629871 1.2702418 ]\n",
      "13920 14 -10633.531203611028 -10628.968783523964 [1.17797391 1.26855796]\n",
      "13920 15 -10633.531203611028 -10629.017339458556 [1.18393874 1.29157625]\n",
      "13920 16 -10646.747958583617 -10646.747958583617 [1.17821456 1.2959975 ]\n",
      "13920 17 -10646.747958583617 -10644.466744225869 [1.17619084 1.30678598]\n",
      "13920 18 -10646.95588394494 -10646.95588394494 [1.17290057 1.32033339]\n",
      "13920 19 -10646.95588394494 -10640.723327768046 [1.16864204 1.3251195 ]\n",
      "13920 20 -10668.1469838715 -10668.1469838715 [1.16519506 1.33307436]\n",
      "13920 21 -10668.1469838715 -10657.67604670292 [1.16788821 1.33533754]\n",
      "13920 22 -10668.1469838715 -10586.046463484718 [1.16995449 1.34688514]\n",
      "13920 23 -10668.1469838715 -10661.777663051553 [1.17263394 1.34967049]\n",
      "13920 24 -10668.1469838715 -10649.43074982569 [1.16923196 1.36436745]\n",
      "13920 25 -10668.1469838715 -10668.124448415427 [1.1672838  1.36222791]\n",
      "13920 26 -10678.336800908184 -10678.336800908184 [1.17189335 1.36689317]\n",
      "13920 27 -10678.336800908184 -10671.623569887946 [1.17018268 1.3683011 ]\n",
      "13920 28 -10678.336800908184 -10677.709252242654 [1.17251486 1.37081495]\n",
      "13920 29 -10678.336800908184 -10623.319086422196 [1.16780338 1.37817399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:56: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:105: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:156: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores: [[-10678.03461811  -7650.07119083 124778.14421774]]\n",
      "validation Scores [[-13873.00540088 -12707.19844711 278668.76158763]]\n",
      "TRAIN: [    0     1     2 ... 27855 27856 27857] TEST: [27858 27859 27860 ... 41784 41785 41786]\n",
      "27840 1 -14156.901444261612 -14156.901444261612 [0.99611304 1.37747235]\n",
      "27840 2 -14156.901444261612 -14045.644251470289 [0.91864846 1.33102575]\n",
      "27840 3 -14164.952863067656 -14164.952863067656 [0.89358474 1.30120768]\n",
      "27840 4 -14164.952863067656 -14139.160629152235 [0.87212114 1.28384112]\n",
      "27840 5 -14192.227966908213 -14192.227966908213 [0.86100912 1.26361726]\n",
      "27840 6 -14192.227966908213 -14181.891623491943 [0.86916401 1.25342306]\n",
      "27840 7 -14198.937134003872 -14198.937134003872 [0.88189197 1.2675225 ]\n",
      "27840 8 -14198.937134003872 -14173.242939861097 [0.88376076 1.26538051]\n",
      "27840 9 -14198.937134003872 -14153.06213963308 [0.87349705 1.26162794]\n",
      "27840 10 -14198.937134003872 -14185.680695524561 [0.88243698 1.26602202]\n",
      "27840 11 -14198.937134003872 -14158.63809372122 [0.88536539 1.26348428]\n",
      "27840 12 -14198.937134003872 -14147.605404570135 [0.88063071 1.27316811]\n",
      "27840 13 -14198.937134003872 -14128.088872715076 [0.89161057 1.26947002]\n",
      "27840 14 -14205.2909911098 -14205.2909911098 [0.89357332 1.27445432]\n",
      "27840 15 -14205.2909911098 -14166.393252705655 [0.88863225 1.27702472]\n",
      "27840 16 -14205.2909911098 -14204.835162222522 [0.89402431 1.2711975 ]\n",
      "27840 17 -14205.2909911098 -14168.40840544776 [0.90105353 1.27744131]\n",
      "27840 18 -14205.2909911098 -14175.166770555785 [0.89864518 1.26729086]\n",
      "27840 19 -14205.2909911098 -14198.091924230292 [0.89027205 1.28584822]\n",
      "27840 20 -14205.2909911098 -14196.796619843884 [0.89322852 1.29728022]\n",
      "27840 21 -14205.2909911098 -14171.245557961694 [0.89955479 1.28473852]\n",
      "27840 22 -14205.2909911098 -14188.131997989318 [0.90631514 1.29389176]\n",
      "27840 23 -14205.2909911098 -14187.824430403627 [0.90191969 1.28652932]\n",
      "27840 24 -14205.2909911098 -14181.63967638736 [0.9009076  1.28988607]\n",
      "27840 25 -14217.176021273624 -14217.176021273624 [0.91847178 1.2879091 ]\n",
      "27840 26 -14217.176021273624 -14188.995951773783 [0.91605496 1.28868858]\n",
      "27840 27 -14217.176021273624 -14193.339044007891 [0.90673134 1.2923211 ]\n",
      "27840 28 -14217.176021273624 -14115.502833388995 [0.9172422  1.28096975]\n",
      "27840 29 -14217.176021273624 -14155.108184043625 [0.92220486 1.29735731]\n",
      "Training scores: [[-10678.03461811  -7650.07119083 124778.14421774]\n",
      " [-14217.46187809 -13840.15955643 267354.32734493]]\n",
      "validation Scores [[-13873.00540088 -12707.19844711 278668.76158763]\n",
      " [-15262.93252067 -15100.55433285 284831.12028625]]\n",
      "TRAIN: [    0     1     2 ... 41784 41785 41786] TEST: [41787 41788 41789 ... 55713 55714 55715]\n",
      "41780 1 -15329.70034116162 -15329.70034116162 [0.88478746 1.2116973 ]\n",
      "41780 2 -15329.70034116162 -15292.826995582367 [0.86812631 1.15751708]\n",
      "41780 3 -15329.70034116162 -15307.175379033575 [0.87825931 1.13104347]\n",
      "41780 4 -15329.70034116162 -15296.54227220577 [0.85220216 1.12910885]\n",
      "41780 5 -15329.70034116162 -15294.231720605409 [0.84162643 1.12024573]\n",
      "41780 6 -15368.488979287884 -15368.488979287884 [0.84202583 1.13505269]\n",
      "41780 7 -15484.318194417396 -15484.318194417396 [0.85761764 1.13071227]\n",
      "41780 8 -15608.252538760607 -15608.252538760607 [0.85143458 1.11175086]\n",
      "41780 9 -15608.252538760607 -15582.417404951564 [0.83108419 1.11690737]\n",
      "41780 10 -15754.43620593275 -15754.43620593275 [0.85320298 1.10258813]\n",
      "41780 11 -15754.43620593275 -15742.760598797464 [0.85408588 1.11341982]\n",
      "41780 12 -15863.146695732079 -15863.146695732079 [0.84661729 1.11979961]\n",
      "41780 13 -15895.419437894721 -15895.419437894721 [0.83512878 1.11137275]\n",
      "41780 14 -15978.25804597087 -15978.25804597087 [0.85227369 1.10672177]\n",
      "41780 15 -16007.568065418469 -16007.568065418469 [0.83497386 1.11027364]\n",
      "41780 16 -16069.909251724685 -16069.909251724685 [0.85675871 1.11200598]\n",
      "41780 17 -16069.909251724685 -16031.32876426048 [0.85443763 1.11913208]\n",
      "41780 18 -16139.31268268236 -16139.31268268236 [0.85224319 1.10546979]\n",
      "41780 19 -16139.31268268236 -16048.531662830532 [0.86282673 1.11580639]\n",
      "41780 20 -16139.31268268236 -16063.251761218511 [0.8683756  1.13013086]\n",
      "41780 21 -16186.91468955702 -16186.91468955702 [0.87442125 1.11959958]\n",
      "41780 22 -16186.91468955702 -16174.379786376996 [0.85689315 1.12707393]\n",
      "41780 23 -16216.722868936298 -16216.722868936298 [0.86631725 1.12928829]\n",
      "41780 24 -16275.032053812485 -16275.032053812485 [0.86802846 1.12343425]\n",
      "41780 25 -16275.032053812485 -16145.92058874712 [0.84875904 1.12943904]\n",
      "41780 26 -16319.08746917514 -16319.08746917514 [0.85650532 1.12600336]\n",
      "41780 27 -16319.08746917514 -16316.915427598295 [0.86826805 1.12324365]\n",
      "41780 28 -16319.08746917514 -16290.997671639981 [0.87362876 1.11483632]\n",
      "41780 29 -16345.326245948676 -16345.326245948676 [0.86284853 1.12877536]\n",
      "Training scores: [[-10678.03461811  -7650.07119083 124778.14421774]\n",
      " [-14217.46187809 -13840.15955643 267354.32734493]\n",
      " [-16331.51210275 -15759.55332789  52412.15572489]]\n",
      "55700 1 -17888.88164855671 -17888.88164855671 [0.85681286 1.08937828]\n",
      "55700 2 -17888.88164855671 -17788.411459818235 [0.87780962 1.06224386]\n",
      "55700 3 -17939.618201490597 -17939.618201490597 [0.85287433 1.06145015]\n",
      "55700 4 -17972.691095058082 -17972.691095058082 [0.85418301 1.05022169]\n",
      "55700 5 -17972.691095058082 -17922.771847784363 [0.84849829 1.06089306]\n",
      "55700 6 -18009.691315765846 -18009.691315765846 [0.83667625 1.07247283]\n",
      "55700 7 -18009.691315765846 -17984.904464794938 [0.84380035 1.07515665]\n",
      "55700 8 -18009.691315765846 -17776.123403980193 [0.84319318 1.06098057]\n",
      "55700 9 -18009.691315765846 -17933.503446489107 [0.84759972 1.05397134]\n",
      "55700 10 -18009.691315765846 -17945.033522755704 [0.87293071 1.05922621]\n",
      "55700 11 -18009.691315765846 -17982.56164146946 [0.84715175 1.05227575]\n",
      "55700 12 -18027.88425466663 -18027.88425466663 [0.83911871 1.05376699]\n",
      "55700 13 -18027.88425466663 -18021.686629321146 [0.8366482  1.07153342]\n",
      "55700 14 -18068.740305808205 -18068.740305808205 [0.85375638 1.04897877]\n",
      "55700 15 -18068.740305808205 -18005.61497628331 [0.86275979 1.05683075]\n",
      "55700 16 -18085.587210104786 -18085.587210104786 [0.85388055 1.06410048]\n",
      "55700 17 -18096.770587967083 -18096.770587967083 [0.84397668 1.06283411]\n",
      "55700 19 -18096.770587967083 -18047.678077124354 [0.86074651 1.0774305 ]\n",
      "55700 20 -18096.770587967083 -18007.246063124938 [0.86283758 1.06424848]\n",
      "55700 21 -18096.770587967083 -18096.713139071744 [0.86122934 1.07011273]\n",
      "55700 22 -18096.770587967083 -18095.34503438902 [0.85175543 1.0706242 ]\n",
      "55700 23 -18096.770587967083 -18072.73797760696 [0.84280653 1.06065893]\n",
      "55700 24 -18096.770587967083 -18078.92052782379 [0.83595028 1.07003235]\n",
      "55700 25 -18096.770587967083 -17942.792475362516 [0.83201933 1.05324666]\n",
      "55700 26 -18096.770587967083 -17947.193340942966 [0.86395661 1.08367754]\n",
      "55700 27 -18096.770587967083 -18025.98753343334 [0.83828235 1.10258498]\n",
      "55700 28 -18138.121638301993 -18138.121638301993 [0.85568375 1.06422212]\n",
      "55700 29 -18174.468405766638 -18174.468405766638 [0.84489413 1.06611226]\n",
      "Training scores: [[-10678.03461811  -7650.07119083 124778.14421774]\n",
      " [-14217.46187809 -13840.15955643 267354.32734493]\n",
      " [-16331.51210275 -15759.55332789  52412.15572489]\n",
      " [-18086.15184712 -15785.40020087  30388.71089574]]\n",
      "validation Scores [[-13873.00540088 -12707.19844711 278668.76158763]\n",
      " [-15262.93252067 -15100.55433285 284831.12028625]\n",
      " [-17748.30242642 -16557.86347643  58784.78667862]\n",
      " [-25392.27605024 -24504.52846277  36152.24026633]]\n",
      "TRAIN: [    0     1     2 ... 69642 69643 69644] TEST: [69645 69646 69647 ... 83571 83572 83573]\n",
      "69640 1 -25466.643035027737 -25466.643035027737 [0.79992722 1.07082698]\n",
      "69640 2 -25466.643035027737 -25429.555388299745 [0.83281549 1.0681884 ]\n",
      "69640 3 -25466.643035027737 -25466.333423438504 [0.80855737 1.06333303]\n",
      "69640 4 -25499.419028717428 -25499.419028717428 [0.82582846 1.04625557]\n",
      "69640 5 -25530.979262379282 -25530.979262379282 [0.80402201 1.04476093]\n",
      "69640 6 -25657.136189583453 -25657.136189583453 [0.80435394 1.07396802]\n",
      "69640 7 -25903.570416571703 -25903.570416571703 [0.80500512 1.04984423]\n",
      "69640 8 -26036.85690418286 -26036.85690418286 [0.80484772 1.04399226]\n",
      "69640 9 -26060.575737231804 -26060.575737231804 [0.82023905 1.03674633]\n",
      "69640 10 -26130.558125262145 -26130.558125262145 [0.81472083 1.0172566 ]\n",
      "69640 11 -26130.558125262145 -26088.008387118884 [0.81435046 1.04051524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69640 12 -26134.68109966719 -26134.68109966719 [0.81807371 1.02168044]\n",
      "69640 13 -26176.944424883564 -26176.944424883564 [0.81375985 1.04984695]\n",
      "69640 14 -26189.967314758993 -26189.967314758993 [0.81686897 1.04260885]\n",
      "69640 15 -26189.967314758993 -26112.952084324428 [0.81614433 1.03735109]\n",
      "69640 16 -26189.967314758993 -26186.68445324117 [0.79967454 1.04294553]\n",
      "69640 17 -26189.967314758993 -26098.466081680403 [0.79847078 1.04177664]\n",
      "69640 18 -26210.66124435423 -26210.66124435423 [0.79679339 1.03366205]\n",
      "69640 19 -26210.66124435423 -26148.261491646786 [0.82040046 1.07648275]\n",
      "69640 20 -26210.66124435423 -26191.805602989596 [0.77171735 1.03971823]\n",
      "69640 21 -26269.37787671317 -26269.37787671317 [0.81809805 1.06701095]\n",
      "69640 22 -26529.235563267746 -26529.235563267746 [0.80902883 1.06316834]\n",
      "69640 23 -26638.801626021934 -26638.801626021934 [0.82305349 1.07390651]\n",
      "69640 24 -26834.141111458386 -26834.141111458386 [0.82111085 1.06021212]\n",
      "69640 25 -26962.32125884073 -26962.32125884073 [0.83881003 1.03697569]\n",
      "69640 26 -27170.121356922715 -27170.121356922715 [0.81078042 1.05381784]\n",
      "69640 27 -27250.284680219247 -27250.284680219247 [0.80921014 1.05439601]\n",
      "69640 28 -27426.262544386725 -27426.262544386725 [0.79204742 1.04459674]\n",
      "69640 29 -27532.73463999894 -27532.73463999894 [0.81484708 1.05278245]\n",
      "Training scores: [[-10678.03461811  -7650.07119083 124778.14421774]\n",
      " [-14217.46187809 -13840.15955643 267354.32734493]\n",
      " [-16331.51210275 -15759.55332789  52412.15572489]\n",
      " [-18086.15184712 -15785.40020087  30388.71089574]\n",
      " [-27466.76586895 -25096.26213528  24781.65244463]]\n",
      "validation Scores [[-13873.00540088 -12707.19844711 278668.76158763]\n",
      " [-15262.93252067 -15100.55433285 284831.12028625]\n",
      " [-17748.30242642 -16557.86347643  58784.78667862]\n",
      " [-25392.27605024 -24504.52846277  36152.24026633]\n",
      " [-40435.61946408 -32786.1378651   21071.8303849 ]]\n"
     ]
    }
   ],
   "source": [
    "from tick.hawkes import HawkesEM, HawkesConditionalLaw\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "nNeurons = 100\n",
    "initializeParams(nNeurons)\n",
    "\n",
    "em = HawkesEM(kernel_support = 6, kernel_size=100, n_threads=1, verbose=False, tol=1e-3)\n",
    "wh = HawkesConditionalLaw(n_quad=200)\n",
    "\n",
    "t_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "score = []\n",
    "scoreTrain =[]\n",
    "\n",
    "\n",
    "for tr_index, ts_index in t_cv.split(sorted_time):\n",
    "    print(\"TRAIN:\",tr_index, \"TEST:\", ts_index)\n",
    "    X_tr, X_val = sorted_time[tr_index], sorted_time[ts_index]\n",
    "    #train\n",
    "    t1 = np.ascontiguousarray(X_tr[X_tr[:,1]==0][:,0]) #select only buy values\n",
    "    t2 = np.ascontiguousarray(X_tr[X_tr[:,1]==1][:,0]) #select only sell values\n",
    "    t = list([t1,t2])\n",
    "    MapT1T2[0] = createMapAtoBIndex(t1,t2)\n",
    "    MapT1T2[1] = createMapAtoBIndex(t2,t1)\n",
    "    SGD = sgdNeuralHawkesBiVariate(30,0.01)\n",
    "    em.fit(t)\n",
    "    wh.fit(t)\n",
    "    scoreTrain.append([SNH_Neg_LL(),EM_Neg_ll(),WH_Neg_ll()])\n",
    "    print('Training scores:',np.array(scoreTrain))\n",
    "    \n",
    "    #test\n",
    "    X_total = np.concatenate([X_tr, X_val])\n",
    "    t1 = np.ascontiguousarray(X_total[X_total[:,1]==0][:,0]) #select only buy values\n",
    "    t2 = np.ascontiguousarray(X_total[X_total[:,1]==1][:,0]) #select only sell values\n",
    "    t = list([t1,t2])\n",
    "    MapT1T2[0] = createMapAtoBIndex(t1,t2)\n",
    "    MapT1T2[1] = createMapAtoBIndex(t2,t1)\n",
    "    snh_ll = SNH_Neg_LL()\n",
    "    em_ll = EM_Neg_ll()\n",
    "    wh_ll = WH_Neg_ll()\n",
    "    score.append([snh_ll,em_ll,wh_ll])\n",
    "    print('validation Scores', np.array(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKernelsAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline                        \n",
    "optimalParams = SGD[0]\n",
    "print('mu_SNH =',optimalParams[4],'mu_EM =', em.baseline,'mu_WH =',wh.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel plot\n",
    "\n",
    "#get node\n",
    "c_nodes = em.n_nodes\n",
    "\n",
    "#tk\n",
    "dx=0.01\n",
    "tk = np.arange(0,5,dx)\n",
    "y2 = np.zeros([len(tk),4])\n",
    "\n",
    "\n",
    "for p in range(4):\n",
    "    y2[:,p] = nnOptimalKernel(tk.reshape(1,-1),p)\n",
    "    \n",
    "\n",
    "\n",
    "#plot\n",
    "fig, ax_list_list = plt.subplots(c_nodes,c_nodes, sharex=True,\n",
    "                                         sharey=True,figsize=(8, 8))\n",
    "    \n",
    "p = 0\n",
    "\n",
    "for i, ax_list in enumerate(ax_list_list):\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        y_values = em.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'b--', label=\"EM\")\n",
    "        y_values = wh.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'g--', label=\"WH\")\n",
    "        y_values = y2[:,p]\n",
    "        ax.plot(tk, y_values,'r--', label=\"SNH\")\n",
    "        p = p+1\n",
    "        \n",
    "        # set x_label for last line\n",
    "        if i == c_nodes - 1:\n",
    "            ax.set_xlabel(r\"$t$\", fontsize=18)\n",
    "\n",
    "        ax.set_ylabel(r\"$\\phi^{%g,%g}(t)$\" % (i, j), fontsize=15)\n",
    "        \n",
    "        legend = ax.legend()\n",
    "        for label in legend.get_texts():\n",
    "            label.set_fontsize(9)\n",
    "plt.axis([-0.1,2.5, -0.2, 4])\n",
    "\n",
    "plt.savefig(\"2D_binance_kernel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
