{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesKernelPowerLaw, SimuHawkes, HawkesKernelTimeFunc\n",
    "from tick.base import TimeFunction\n",
    "\n",
    "A=[]\n",
    "B=[]\n",
    "A_grad=[]\n",
    "B_grad=[]\n",
    "tseriesA = np.array([])\n",
    "epsilon = 1e-8\n",
    "#delta=1\n",
    "#alpha=1\n",
    "#beta=4\n",
    "#mu=0.05\n",
    "Dict={}\n",
    "Dict_gradient={}\n",
    "Dict_integrate={}\n",
    "#power kernel = alpha*(delta+t)^(-beta)\n",
    "\n",
    "#simulation part\n",
    "#removed\n",
    "    \n",
    "def inflectionPoints():\n",
    "   \n",
    "    div = B[0]+epsilon*(np.abs(B[0])<epsilon)\n",
    "    x = -B[1]/div\n",
    "    interestX1 = x*(x>0)\n",
    "    alwaysInclude = (x<=0)*(B[0]>0) #dont change\n",
    "    alwaysExclude = (x<=0)*(B[0]<0)\n",
    "    tempX = x*(~alwaysInclude)*(~alwaysExclude)\n",
    "    interestX1 = tempX[interestX1>0]\n",
    "    interestX = np.sort(interestX1)\n",
    "    interestX = np.append(0,interestX)\n",
    "    \n",
    "    Dict['inflection'] = interestX\n",
    "   \n",
    "    con = A[0]*B[0]\n",
    "    Dict['constant'] = con\n",
    "    return\n",
    "\n",
    "def loglikelihood(para):\n",
    "    alpha=para[0]\n",
    "    beta =para[1]\n",
    "    mu=para[2]\n",
    "    \n",
    "    tend = t[-1]-t\n",
    "    \n",
    "    ll= mu*t[-1]+(alpha/beta)*len(t)\n",
    "    ll2 = np.sum(-(alpha/beta)*np.exp(-beta*tend))\n",
    "    ll = ll+ll2-np.log(mu)\n",
    "    print('exp',ll,(alpha/beta)*len(t))\n",
    "    for i in range(2,len(t)+1,1):\n",
    "        li = max(i-10,0)\n",
    "        temp = t[i-1]-t[li:i-1]\n",
    "        \n",
    "        logLam = -np.log(mu+np.sum(alpha*np.exp(-beta*temp)))\n",
    "        ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "def loglikelihood_rect(para):\n",
    "    alpha=para[0]\n",
    "    beta =para[1]\n",
    "    delta = para[2]\n",
    "    mu=para[3]\n",
    "    \n",
    "   \n",
    "    \n",
    "    tend = t[-1]-t\n",
    "    condition1 = (tend>(delta+1/beta))\n",
    "    condition2 = (tend>delta)*(tend<(delta+1/beta))\n",
    "    ll= mu*t[-1]\n",
    "    ll2 = np.sum(alpha*(condition1)+alpha*beta*(tend-delta)*(condition2))\n",
    "    ll = ll+ll2-np.log(mu)\n",
    "    llinit=ll\n",
    "   \n",
    "    for i in range(2,len(t)+1,1):\n",
    "        li = max(i-10,0)*0\n",
    "        temp = t[i-1]-t[li:i-1]\n",
    "        \n",
    "        logLam = -np.log(mu+np.sum(alpha*beta*((temp>delta)*(temp<(delta+1/beta)))))\n",
    "        ll = ll+logLam\n",
    "    print('rect',ll2,llinit+np.log(mu), ll-llinit-np.log(mu))\n",
    "    return ll\n",
    "\n",
    "def gradientll(para,iArray):\n",
    "    alpha=para[0]\n",
    "    beta =para[1]\n",
    "    mu=para[2]\n",
    "    tend = t[-1]-t\n",
    "    grad = np.array([0.0, 0.0, 0.0])\n",
    "    for i  in np.nditer(iArray):\n",
    "        li = max(i-50,0)\n",
    "        temp = t[i]-t[li:i]\n",
    "        decayFactor = sum(np.exp(-beta*temp))\n",
    "        lam = mu+alpha*decayFactor\n",
    "        fac1 = np.exp(-beta*tend[i])\n",
    "        fac2 = (alpha/beta)*(-(1/beta)+fac1*((1/beta)+tend[i]))\n",
    "        fac3 = (alpha/lam)*(sum(temp*np.exp(-beta*temp)))+fac2\n",
    "        grad[0] = grad[0]+(1/beta)*(1-fac1)-(1/lam)*decayFactor\n",
    "        grad[1]= grad[1]+fac3\n",
    "        grad[2] = grad[2]+ ((t[i]-t[i-1])-(1/lam))*(i >0)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    return grad/len(iArray)\n",
    "\n",
    "def sgdHawkes(para):\n",
    "    alpha=para[0]\n",
    "    beta =para[1]\n",
    "    mu=para[2]\n",
    "    lr =0.01\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "    epsilon = 1e-8\n",
    "    count = 0\n",
    "    bestll = 1e8\n",
    "    bestpara = np.array([0.,0.,0.])\n",
    "    m_t = np.array([0.,0.,0.])\n",
    "    v_t = np.array([0.,0.,0.])\n",
    "    for epochs in range(1,30,1):\n",
    "        \n",
    "        rsample = np.random.choice(len(t),len(t)-1,replace = False)\n",
    "        for i in range(1,len(rsample),50):\n",
    "            count=count+1 \n",
    "            grad = gradientll(para,rsample[i:i+50])\n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad*grad)\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            para = para-(lr*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            \n",
    "            error=loglikelihood(para)\n",
    "            bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            para = np.maximum(para,0)\n",
    "            bestll = min(bestll,error)\n",
    "            \n",
    "            print(i,error,bestll,para,bestpara)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara  \n",
    "    \n",
    "\n",
    "def sigmoid(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
    "    \n",
    "def nnIntegratedKernel(x):\n",
    "    x = x.reshape(-1)\n",
    "    alphas = A[0]\n",
    "    alpha0 = A[1]\n",
    "    betas = B[0]\n",
    "    beta0 = B[1]\n",
    "    const1 = Dict.get('constant')\n",
    "    interestX = Dict.get('inflection')\n",
    "    precalculate_integrate()\n",
    "    \n",
    "    y = np.zeros([1,len(x)])\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        xi = x[i]\n",
    "        if(xi>0):\n",
    "            iP = max(interestX[interestX<xi])\n",
    "            n1 = betas*(xi-epsilon) + beta0\n",
    "            dn1 = (n1>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "            term1 = nnKernel(xi)*((const!=0)+xi*(const==0))\n",
    "            term2 = nnKernel(iP)*((const!=0)+iP*(const==0))\n",
    "            \n",
    "            const = (const)*(const!=0)+(const==0)*1.0\n",
    "            \n",
    "            prev_term = Dict_integrate.get(iP)\n",
    "            \n",
    "            y[0,i] = prev_term + ((term1-term2)/(const))\n",
    "        \n",
    "       \n",
    "               \n",
    "\n",
    "    return y\n",
    "\n",
    "def nnKernel(x):\n",
    "    alphas = A[0]\n",
    "    alpha0 = A[1]\n",
    "    betas = B[0]\n",
    "    beta0 = B[1]\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y    \n",
    "\n",
    "\n",
    "def initializeParams(nNeurons):\n",
    "    alphas = -(np.random.uniform(0,1,nNeurons)).reshape(-1,1)*0.5\n",
    "    alpha0 = -np.random.uniform(0,1,1)*0.5\n",
    "    betas = (np.random.uniform(0,1,nNeurons)).reshape(-1,1)*0.5\n",
    "    beta0 = np.random.uniform(0,1,nNeurons).reshape(-1,1)*0.05\n",
    "    mu =np.random.uniform(0,1,1)\n",
    "    \n",
    "    A.append(alphas)\n",
    "    B.append(betas)\n",
    "    A.append(alpha0)\n",
    "    B.append(beta0)\n",
    "    A_grad.append(np.zeros([nNeurons,1]))\n",
    "    A_grad.append(0)\n",
    "    B_grad.append(np.zeros([nNeurons,1]))\n",
    "    B_grad.append(np.zeros([nNeurons,1]))\n",
    "    return mu\n",
    "\n",
    "def precalculate_integrate():\n",
    "    alphas = A[0]\n",
    "    alpha0 = A[1]\n",
    "    betas = B[0]\n",
    "    beta0 = B[1]\n",
    "    iP = Dict.get('inflection')\n",
    "    const1 = Dict.get('constant')\n",
    "    Dict_integrate.clear()\n",
    "    Dict_integrate[0]=0\n",
    "    y = 0\n",
    "    for index in range(1,len(iP)):\n",
    "        n1 = betas*(iP[index]-epsilon) + beta0\n",
    "        dn1 = (n1>0)\n",
    "        const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "        term1 = nnKernel(iP[index])*((const!=0)+iP[index]*(const==0))\n",
    "        term2 = nnKernel(iP[index-1])*((const!=0)+iP[index-1]*(const==0))\n",
    "            \n",
    "        const = (const)*(const!=0)+(const==0)*1.0\n",
    "              \n",
    "        y= y + ((term1-term2)/(const))\n",
    "        Dict_integrate[iP[index]]=y\n",
    "    return\n",
    "    \n",
    "    \n",
    "def precalculate_gradient():\n",
    "    alphas = A[0]\n",
    "    alpha0 = A[1]\n",
    "    betas = B[0]\n",
    "    beta0 = B[1]\n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    iP = Dict.get('inflection')\n",
    "    \n",
    "    const1 = Dict.get('constant')\n",
    "    Dict_gradient.clear()\n",
    "    Dict_gradient[0]=list([gradA0,gradA,gradB1,gradB0])\n",
    "    for index in range(1,len(iP)):\n",
    "            \n",
    "            n0pr = betas*(iP[index-1]+epsilon)+beta0\n",
    "            n1pr = betas*(iP[index]-epsilon) + beta0\n",
    "            dn1 = (n1pr>0)\n",
    "            dn0 = (n0pr>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            indicator = const==0\n",
    "            const = const*(const!=0)+1*(const==0)\n",
    "            n0 = betas*(iP[index-1])+beta0\n",
    "            n1 = betas*(iP[index]) + beta0\n",
    "            \n",
    "            fac1 = nnKernel(iP[index])\n",
    "            fac2 = nnKernel(iP[index-1])\n",
    "            gradA0 = gradA0 + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(iP[index]-iP[index-1])\n",
    "            gradA = gradA -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            gradB1 = gradB1 -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradB1 = gradB1 + ((1/const)*(alphas)*(fac1*iP[index]*dn1-fac2*iP[index-1]*dn0))*(~indicator)\n",
    "            gradB0 = gradB0+ ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "            Dict_gradient[iP[index]]= list([gradA0,gradA,gradB1,gradB0])\n",
    "    \n",
    "    return\n",
    "        \n",
    "            \n",
    "         \n",
    "    \n",
    "def gradientNetwork(iArray,mu):\n",
    "    alphas = A[0]\n",
    "    alpha0 = A[1]\n",
    "    betas = B[0]\n",
    "    beta0 = B[1]\n",
    "    precalculate_gradient()\n",
    "    tend = t[-1]-t\n",
    "    x = tend[iArray]\n",
    "    const1 = Dict.get('constant')\n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    grad_mu = 0\n",
    "    \n",
    "    interestX = Dict.get('inflection')\n",
    "    temp = 0\n",
    "    for xi in np.nditer(x):\n",
    "        if(xi>0):\n",
    "            iP = max(interestX[interestX<xi])\n",
    "            n1pr = betas*(xi-epsilon) + beta0\n",
    "            n0pr = betas*(iP+epsilon)+beta0\n",
    "           \n",
    "            dn1 = (n1pr>0)\n",
    "            dn0 = (n0pr>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            indicator = const==0\n",
    "            const = const*(const!=0)+1*(const==0)\n",
    "            n0 = betas*(iP)+beta0\n",
    "            n1 = betas*(xi) + beta0\n",
    "            fac1 = nnKernel(xi)\n",
    "            fac2 = nnKernel(iP)\n",
    "            gradients = Dict_gradient.get(iP)\n",
    "            \n",
    "            \n",
    "            gradA0 = gradA0+gradients[0] + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(xi-iP)\n",
    "            \n",
    "            gradA = gradA + gradients[1] -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            \n",
    "            gradB1 = gradB1+gradients[2] -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradB1 = gradB1+ ((1/const)*(alphas)*(fac1*xi*dn1-fac2*iP*dn0))*(~indicator)\n",
    "    \n",
    "            gradB0 =gradB0+ gradients[3] + ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "        \n",
    "         \n",
    "    for i in np.nditer(iArray):\n",
    "        li = max(i-40,0)\n",
    "        temp = t[i]-t[li:i]\n",
    "        fac1 = nnKernel(temp.reshape(1,-1))\n",
    "        decayFactor = np.sum(fac1)\n",
    "        lam = mu+decayFactor\n",
    "        invLam = (1/lam)\n",
    "        n1 = np.dot(betas,temp.reshape(1,-1)) + beta0\n",
    "        gradA = gradA-np.sum(invLam*(fac1*np.maximum(n1,0)),axis=1).reshape(-1,1)\n",
    "        \n",
    "        gradA0 = gradA0 - invLam*decayFactor\n",
    "        \n",
    "        gradB1 = gradB1 - invLam*np.sum(fac1*(n1>0)*alphas*temp.reshape(1,-1),axis=1).reshape(-1,1)\n",
    "        gradB0 = gradB0 - invLam*np.sum(fac1*(n1>0)*alphas,axis=1).reshape(-1,1)\n",
    "        grad_mu = grad_mu + ((t[i]-t[i-1])-invLam)*(i >0)\n",
    "        \n",
    "    gradA = gradA/len(iArray)\n",
    "    gradB1 = gradB1/len(iArray)\n",
    "    gradB0 = gradB0/len(iArray)\n",
    "    gradA0 = gradA0/len(iArray)\n",
    "    A_grad[0] = gradA\n",
    "    A_grad[1] = gradA0\n",
    "    B_grad[0] = gradB1\n",
    "    B_grad[1] = gradB0\n",
    "    return (grad_mu/len(iArray))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plotKernels():\n",
    "   \n",
    "    \n",
    "    dx=0.01\n",
    "    tk = np.arange(0,5,dx)\n",
    "    interestX = Dict.get('inflection')\n",
    "    \n",
    "    plt.plot(tk,nnKernel(tk.reshape(1,-1)).reshape(-1,1),'r--')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    ytemp = nnKernel(tk.reshape(1,-1))\n",
    "    intYtemp = np.cumsum(ytemp*dx)\n",
    "\n",
    "\n",
    "    plt.plot(tk,intYtemp.reshape(-1,1),'k--')\n",
    "    plt.plot(tk,nnIntegratedKernel(tk.reshape(1,-1)).reshape(-1,1),'r--')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.pause(0.0005)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def nnLoglikelihood(mu):\n",
    "    \n",
    "    \n",
    "    tend = (t[-1]-t)\n",
    "    a = np.sum(nnIntegratedKernel(tend.reshape(1,-1)))\n",
    "   \n",
    "    ll = mu*t[-1]+a\n",
    "    ll = ll-np.log(mu)\n",
    "    llInit = ll+np.log(mu)\n",
    "    \n",
    "    nElements = max(t.shape)\n",
    "    \n",
    "    \n",
    "    for i in range(2,nElements+1,1):\n",
    "        li = max(i-40,0)\n",
    "        temp = t[i-1]-t[li:i-1]\n",
    "        \n",
    "        z = np.sum(nnKernel(temp.reshape(1,-1)))\n",
    "        \n",
    "        \n",
    "        logLam = -np.log(mu+z)\n",
    "        \n",
    "        ll = ll+logLam\n",
    "    #print('nn',a,llInit,ll-llInit-np.log(mu))\n",
    "    return ll\n",
    "\n",
    "\n",
    " \n",
    "def sgdNeuralHawkes(nEpochs,lr,mu):\n",
    "    \n",
    "    lr2 =lr*0.1\n",
    "    lr_mu = lr*0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "  \n",
    "    \n",
    "    bestll = 1e8\n",
    "    neg_ll = []\n",
    "    \n",
    "    m_t_A = np.zeros([nNeurons,1])\n",
    "    m_t_A0 =0\n",
    "    m_t_B= np.zeros([nNeurons,1])\n",
    "    m_t_B0= np.zeros([nNeurons,1])\n",
    "    m_t = 0\n",
    "    v_t_A = np.zeros([nNeurons,1])\n",
    "    v_t_A0 =0\n",
    "    v_t_B= np.zeros([nNeurons,1])\n",
    "    v_t_B0= np.zeros([nNeurons,1])\n",
    "    v_t = 0\n",
    "    count = 0\n",
    "    for epochs in range(1,nEpochs+1,1):\n",
    "        \n",
    "        inflectionPoints()\n",
    "           \n",
    "        rsample = np.random.choice(len(t),len(t)-1,replace = False)\n",
    "        for i in range(1,len(rsample),50):\n",
    "            count=count+1 \n",
    "            \n",
    "            grad = gradientNetwork(rsample[i:i+50],mu)\n",
    "            \n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad*grad)\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            mu = mu-(lr_mu*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            mu = np.maximum(mu,0)\n",
    "        \n",
    "            \n",
    "            m_t_A = beta_1*m_t_A + (1-beta_1)*A_grad[0]\t#updates the moving averages of the gradient\n",
    "            v_t_A = beta_2*v_t_A + (1-beta_2)*(A_grad[0]*A_grad[0])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A = m_t_A/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A = v_t_A/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            A[0] = A[0]-(lr*m_cap_A)/(np.sqrt(v_cap_A)+epsilon)\n",
    "            \n",
    "            \n",
    "            m_t_A0 = beta_1*m_t_A0 + (1-beta_1)*A_grad[1]\t#updates the moving averages of the gradient\n",
    "            v_t_A0 = beta_2*v_t_A0 + (1-beta_2)*(A_grad[1]*A_grad[1])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A0 = m_t_A0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A0 = v_t_A0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            A[1] = A[1]-(lr*m_cap_A0)/(np.sqrt(v_cap_A0)+epsilon)\n",
    "            \n",
    "                \n",
    "            m_t_B = beta_1*m_t_B + (1-beta_1)*B_grad[0]\t#updates the moving averages of the gradient\n",
    "            v_t_B = beta_2*v_t_B + (1-beta_2)*(B_grad[0]*B_grad[0])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B = m_t_B/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B= v_t_B/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            B[0] = B[0]-(lr2*m_cap_B)/(np.sqrt(v_cap_B)+epsilon)\n",
    "            \n",
    "            \n",
    "            m_t_B0 = beta_1*m_t_B0 + (1-beta_1)*B_grad[1]\t#updates the moving averages of the gradient\n",
    "            v_t_B0 = beta_2*v_t_B0 + (1-beta_2)*(B_grad[1]*B_grad[1])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B0 = m_t_B0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B0 = v_t_B0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            B[1] = B[1]-(lr2*m_cap_B0)/(np.sqrt(v_cap_B0)+epsilon)\n",
    "          \n",
    "            inflectionPoints()\n",
    "            \n",
    "        error=nnLoglikelihood(mu)\n",
    "        neg_ll.append(error)\n",
    "        #bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            \n",
    "        if(bestll > error):\n",
    "            A0 = np.array(A[0])\n",
    "            A1 =np.array(A[1])\n",
    "            B0 = np.array(B[0])\n",
    "            B1= np.array(B[1])\n",
    "            optimalParams=list([A0,A1,B0,B1,mu])\n",
    "               \n",
    "            \n",
    "        bestll = min(bestll,error)\n",
    "        print(i,epochs,bestll,error,mu)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara\n",
    "            \n",
    "            \n",
    "        #plotKernels()\n",
    "    return optimalParams, neg_ll, nEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate Neg_ll of SNH\n",
    "def nnOptimalKernel(x):\n",
    "    optimalParams = SGD[0]\n",
    "    alphas = optimalParams[0].reshape(-1,1)\n",
    "    alpha0 = optimalParams[1].reshape(-1,1)\n",
    "    betas = optimalParams[2].reshape(-1,1)\n",
    "    beta0 = optimalParams[3].reshape(-1,1)\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y  \n",
    "\n",
    "def customIntegrateKernel():\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx) \n",
    "    tend = (t[-1]-t)\n",
    "    \n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    \n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = nnOptimalKernel(t3.reshape(1,-1)).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def SNH_Neg_LL():\n",
    "    optimalParams = SGD[0]\n",
    "    mu = optimalParams[4]\n",
    "    \n",
    "    tend = (t[-1]-t)\n",
    "    \n",
    "    a = np.sum(customIntegrateKernel())\n",
    "    #print(a)\n",
    "    ll = mu*t[-1]+a\n",
    "    ll = ll-np.log(mu)\n",
    "    llInit = ll+np.log(mu)\n",
    "    \n",
    "    nElements = max(t.shape)\n",
    "        \n",
    "    for i in range(2,nElements+1,1):\n",
    "        li = max(i-40,0)\n",
    "        temp = t[i-1]-t[li:i-1]      \n",
    "        z = np.sum(nnOptimalKernel(temp.reshape(1,-1)))               \n",
    "        logLam = -np.log(mu+z)        \n",
    "        ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "#to calculate Neg_ll of EM\n",
    "\n",
    "def customIntegrateKernelEM():\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    tend = (t[-1]-t)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    ytemp = em.get_kernel_values(0,0,t3.reshape(1,-1)).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def EM_Neg_ll():\n",
    "    mu = em.baseline\n",
    "    a = np.sum(customIntegrateKernelEM())\n",
    "    ll = mu*t[-1]+a\n",
    "    ll = ll-np.log(mu)\n",
    "    llInit = ll+np.log(mu)\n",
    "    nElements = max(t.shape)\n",
    "    for i in range(2,nElements+1,1):\n",
    "        li = max(i-40,0)\n",
    "        temp = t[i-1]-t[li:i-1]        \n",
    "        z = np.sum(em.get_kernel_values(0,0,temp))\n",
    "        logLam = -np.log(mu+z)        \n",
    "        ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "#to calculate Neg_ll of WH\n",
    "\n",
    "def customIntegrateKernelWH():\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    tend = (t[-1]-t)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = wh.get_kernel_values(0,0,t3.reshape(1,-1)).reshape(-1)\n",
    "    #ytemp[ytemp < 0] = 0\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def WH_Neg_ll():\n",
    "    mu = wh.baseline\n",
    "    a = np.sum(customIntegrateKernelWH())\n",
    "   \n",
    "    ll = mu*t[-1]+a\n",
    "    ll = ll-np.log(mu)\n",
    "    llInit = ll+np.log(mu)\n",
    "    \n",
    "    nElements = max(t.shape)\n",
    "    \n",
    "    for i in range(2,nElements+1,1):\n",
    "        li = max(i-40,0)\n",
    "        temp = t[i-1]-t[li:i-1]        \n",
    "        temp2 = wh.get_kernel_values(0,0,temp)\n",
    "        temp2[temp2 < 0] = 0\n",
    "        z = np.sum(temp2)\n",
    "        logLam = -np.log(mu+z)        \n",
    "        ll = ll+logLam\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 46946\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/lekhapriya/SNH/master/Combined.csv\"\n",
    "dataset = pd.read_csv(url, names=['Timestamp', 'Price','Volume','Buyer ID','Seller ID','Buyer is market maker'])\n",
    "dataset = dataset.sort_values(dataset.columns[0])\n",
    "\n",
    "orderType = 0  #0--->sell, 1--->buy\n",
    "\n",
    "if(orderType == 0):\n",
    "    result_df = dataset.loc[dataset.iloc[:,5]]\n",
    "    result = result_df.drop_duplicates(subset=['Seller ID'])\n",
    "elif(orderType == 1):\n",
    "    result_df = dataset.loc[~dataset.iloc[:,5]]\n",
    "    result = result_df.drop_duplicates(subset=['Buyer ID'])\n",
    "\n",
    "#ms to seconds\n",
    "utc = (result.iloc[:,0].values)/1000\n",
    "\n",
    "#convert utc to seconds since t0\n",
    "t_ms = []\n",
    "for i in range(0,len(utc)) :\n",
    "    t_ms.append(utc[i]-utc[0])\n",
    "    \n",
    "timestamp = np.array(t_ms)\n",
    "print('N =',len(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 7823 7824 7825] TEST: [ 7826  7827  7828 ... 15647 15648 15649]\n",
      "7801 1 [-5686.64821425] [-5686.64821425] [0.1939603]\n",
      "7801 2 [-5921.56821586] [-5921.56821586] [0.25998892]\n",
      "7801 3 [-6080.23037821] [-6080.23037821] [0.32797256]\n",
      "7801 4 [-6149.10543632] [-6149.10543632] [0.394161]\n",
      "7801 5 [-6164.26652921] [-6164.26652921] [0.45725556]\n",
      "7801 6 [-6299.39001973] [-6299.39001973] [0.51633518]\n",
      "7801 7 [-6475.01818892] [-6475.01818892] [0.57512411]\n",
      "7801 8 [-6585.34966118] [-6585.34966118] [0.62975703]\n",
      "7801 9 [-6694.57714495] [-6694.57714495] [0.67814417]\n",
      "7801 10 [-6737.24612636] [-6737.24612636] [0.72412297]\n",
      "7801 11 [-6778.43050221] [-6778.43050221] [0.77069604]\n",
      "7801 12 [-6812.3123996] [-6812.3123996] [0.81343888]\n",
      "7801 13 [-6832.93249851] [-6832.93249851] [0.85597164]\n",
      "7801 14 [-6858.12025993] [-6858.12025993] [0.90317652]\n",
      "7801 15 [-6870.77081959] [-6870.77081959] [0.94521673]\n",
      "7801 16 [-6885.53093805] [-6885.53093805] [0.98769012]\n",
      "7801 17 [-6902.53852875] [-6902.53852875] [1.02926728]\n",
      "7801 18 [-6914.81105424] [-6914.81105424] [1.06636148]\n",
      "7801 19 [-6928.44228107] [-6928.44228107] [1.10195419]\n",
      "7801 20 [-6930.1298292] [-6930.1298292] [1.13472915]\n",
      "7801 21 [-6932.05541472] [-6932.05541472] [1.16868797]\n",
      "7801 22 [-6934.5823436] [-6934.5823436] [1.19661236]\n",
      "7801 23 [-6945.68027211] [-6945.68027211] [1.2266232]\n",
      "7801 24 [-6945.68027211] [-6938.39501037] [1.24971229]\n",
      "7801 25 [-6947.95367338] [-6947.95367338] [1.27570745]\n",
      "7801 26 [-6952.95394091] [-6952.95394091] [1.29721444]\n",
      "7801 27 [-6953.54339238] [-6953.54339238] [1.31522342]\n",
      "7801 28 [-6955.948907] [-6955.948907] [1.32697218]\n",
      "7801 29 [-6958.99520551] [-6958.99520551] [1.34600071]\n",
      "7801 30 [-6962.35846648] [-6962.35846648] [1.35635789]\n",
      "Training scores: [[[  -6962.32472111]\n",
      "  [  -5319.96208846]\n",
      "  [-110880.94195351]]]\n",
      "validation Scores [[[ -9394.85308789]\n",
      "  [ -8362.42056824]\n",
      "  [-38704.05350622]]]\n",
      "TRAIN: [    0     1     2 ... 15647 15648 15649] TEST: [15650 15651 15652 ... 23471 23472 23473]\n",
      "15601 1 [-9027.59357024] [-9027.59357024] [0.32635507]\n",
      "15601 2 [-9192.74417531] [-9192.74417531] [0.50565413]\n",
      "15601 3 [-9380.77643199] [-9380.77643199] [0.66215054]\n",
      "15601 4 [-9437.67353105] [-9437.67353105] [0.79688116]\n",
      "15601 5 [-9492.87806059] [-9492.87806059] [0.90772566]\n",
      "15601 6 [-9518.17511877] [-9518.17511877] [1.00509431]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4cc92bd392ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0minflectionPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mSGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdNeuralHawkes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f57436199752>\u001b[0m in \u001b[0;36msgdNeuralHawkes\u001b[0;34m(nEpochs, lr, mu)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m  \u001b[0;31m#updates the moving averages of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f57436199752>\u001b[0m in \u001b[0;36mgradientNetwork\u001b[0;34m(iArray, mu)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mgradA0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradA0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minvLam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdecayFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mgradB1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradB1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minvLam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfac1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mgradB0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradB0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minvLam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfac1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mgrad_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_mu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minvLam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tick.hawkes import HawkesEM, HawkesConditionalLaw\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "nNeurons=100\n",
    "support = 100\n",
    "em = HawkesEM(kernel_support = 6, kernel_size=100, n_threads=4, verbose=False, tol=1e-4)\n",
    "wh = HawkesConditionalLaw(n_quad=200)\n",
    "\n",
    "t_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "score = []\n",
    "scoreTrain =[]\n",
    "A=[]\n",
    "B=[]\n",
    "A_grad=[]\n",
    "B_grad=[]\n",
    "Dict={}\n",
    "Dict_gradient={}\n",
    "Dict_integrate={}\n",
    "x = initializeParams(nNeurons)\n",
    "for tr_index, ts_index in t_cv.split(timestamp):\n",
    "    print(\"TRAIN:\", tr_index, \"TEST:\", ts_index)\n",
    "    X_tr, X_val = timestamp[tr_index], timestamp[ts_index]\n",
    "    #train\n",
    "    \n",
    "    \n",
    "    inflectionPoints()\n",
    "    t = X_tr\n",
    "    SGD = sgdNeuralHawkes(30,0.01,x)\n",
    "    t = t.astype('float64') \n",
    "    em.fit([t])\n",
    "    wh.fit([t])\n",
    "    scoreTrain.append([SNH_Neg_LL(),EM_Neg_ll(),WH_Neg_ll()])\n",
    "    print('Training scores:',np.array(scoreTrain))\n",
    "    \n",
    "    #test\n",
    "    t = np.concatenate([X_tr, X_val])\n",
    "    snh_ll = SNH_Neg_LL()\n",
    "    em_ll = EM_Neg_ll()\n",
    "    wh_ll = WH_Neg_ll()\n",
    "    score.append([snh_ll,em_ll,wh_ll])\n",
    "    print('validation Scores', np.array(score))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel plot\n",
    "\n",
    "from tick.hawkes import HawkesEM, HawkesConditionalLaw\n",
    "\n",
    "\n",
    "em = HawkesEM(kernel_support = 6, kernel_size=100, n_threads=4, verbose=False, tol=1e-4)\n",
    "wh = HawkesConditionalLaw(n_quad=200)\n",
    "#wh = HawkesConditionalLaw(claw_method=\"log\", delta_lag=0.1, min_lag=0.002,\n",
    "                         #max_lag=100, quad_method=\"log\", n_quad=100,\n",
    "                         #min_support=0.002, max_support=support, n_threads=-1)\n",
    "\n",
    "\n",
    "timestamp = timestamp.astype('float64') \n",
    "h_time = [timestamp]\n",
    "\n",
    "em.fit(h_time)\n",
    "wh.fit(h_time)\n",
    "#wh.incremental_fit(h_time)\n",
    "#wh.compute()\n",
    "\n",
    "#to learn mu\n",
    "optimalParams = SGD[0]\n",
    "print('mu_SNH =',optimalParams[4],'mu_EM =', em.baseline,'mu_WH =',wh.baseline)\n",
    "\n",
    "#checking the attributes\n",
    "c_nodes = em.n_nodes\n",
    "\n",
    "dx=0.01\n",
    "tk = np.arange(0,5,dx)\n",
    "\n",
    "fig, axs = plt.subplots(c_nodes,c_nodes, sharex=True,\n",
    "                                         sharey=True,figsize=(6, 6))\n",
    "\n",
    "    \n",
    "ax_list_list = np.array([[axs]])\n",
    "\n",
    "for i, ax_list in enumerate(ax_list_list):\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        y_values = em.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'b-', label='EM')\n",
    "        y_values = wh.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'g-', label='WH')\n",
    "\n",
    "axs.plot(tk,nnOptimalKernel(tk.reshape(1,-1)).reshape(-1,1),'r-', label = 'SNH')\n",
    "axs.grid()\n",
    "plt.legend(fontsize=10)\n",
    "plt.title('training set kernel')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('\\u03C6')\n",
    "plt.axis([-0.2, 5, -0.2,3])\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"1D_binance_kernel_train.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negll = SGD[1]\n",
    "plt.plot(range(0,len(negll)),negll,'k-')\n",
    "min(negll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WH_Neg_ll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
