{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesKernelPowerLaw, SimuHawkes, HawkesKernelTimeFunc\n",
    "from tick.base import TimeFunction\n",
    "optimalParams = []\n",
    "nNeurons = 100\n",
    "\n",
    "Alphas = np.zeros([nNeurons,4])\n",
    "Alpha0 = -np.zeros([1,4])\n",
    "Betas = np.zeros([nNeurons,4])\n",
    "Beta0 = np.zeros([nNeurons,4])\n",
    "mu = np.random.uniform(0,1,2)\n",
    "\n",
    "epsilon = 1e-8\n",
    "\n",
    "Dict={x: {} for x in range(4)}\n",
    "Dict_integrate = {x: {} for x in range(4)}  #nested dictionary for all networks\n",
    "Dict_gradient = {x: {} for x in range(4)}\n",
    "\n",
    "MapT1T2={}\n",
    "adjacentKernel={0:[0,1],1:[3,2]}\n",
    "para = np.zeros([4,2])\n",
    "#power kernel = alpha*(delta+t)^(-beta)\n",
    "\n",
    "#simulation part\n",
    "\n",
    "\n",
    "\n",
    "def createMapAtoBIndex(a,b):\n",
    "    mapAtoBIndex={}\n",
    "    for x in a:\n",
    "        if(max(b[b<x],default=-1)==-1):\n",
    "            mapAtoBIndex[x] = None\n",
    "        else:\n",
    "            mapAtoBIndex[x] = (np.where(b==max(b[b<x])))\n",
    "    return mapAtoBIndex\n",
    "    \n",
    "\n",
    "\n",
    "def exponentialIntegratedKernel(tend,kernel):\n",
    "    alpha = para[kernel,0]\n",
    "    beta = para[kernel,1]\n",
    "    \n",
    "    res = (alpha/beta)*(1-np.exp(-beta*tend))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def exponentialKernel(tp,kernel):\n",
    "    alpha = para[kernel,0]\n",
    "    beta = para[kernel,1]\n",
    "    res = alpha*np.exp(-beta*tp)\n",
    "   \n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def loglikelihood():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(exponentialIntegratedKernel(tend.reshape(1,-1),p))\n",
    "        \n",
    "        a = a + np.sum(exponentialIntegratedKernel(tend.reshape(1,-1),p+2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ll = ll+ mu[p]*T+a\n",
    "       \n",
    "        \n",
    "        ll = ll-np.log(mu[p])\n",
    "       \n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-30,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(exponentialKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0])\n",
    "                lj = max(j-30,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(exponentialKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            \n",
    "            ll = ll+logLam\n",
    "        \n",
    "    return ll\n",
    "\n",
    "def gradientIntegratedKernel(tend,p):\n",
    "    alpha = para[p,0]\n",
    "    beta = para[p,1]\n",
    "    fac1 = np.exp(-beta*tend)\n",
    "    val = np.zeros([2])\n",
    "    val[0]=(1/beta)*(1-fac1)\n",
    "   \n",
    "    fac2 = (alpha/beta)*(-(1/beta)+fac1*((1/beta)+tend))\n",
    "    val[1] = fac2\n",
    "    return val\n",
    "\n",
    "def gradientExpKernel(temp,p):\n",
    "    alpha = para[p,0]\n",
    "    beta = para[p,1]\n",
    "    fac1 = np.exp(-beta*temp)\n",
    "    val = np.zeros([2])\n",
    "    val[0]=np.sum(fac1)\n",
    "    val[1]= -alpha*np.sum(temp*fac1)\n",
    "    return val\n",
    "    \n",
    "\n",
    "def gradientll(iArray):\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    \n",
    "    grad = np.zeros([4,2])\n",
    "    for i in range(0,max(iArray.shape),1):\n",
    "        factor1=0\n",
    "        factor2=0\n",
    "        p = iArray[0,i]\n",
    "        index = iArray[1,i]\n",
    "        tend = (T-t[p][index])\n",
    "        grad[p,:]=grad[p,:]+gradientIntegratedKernel(tend,p)\n",
    "        \n",
    "        grad[p+2,:]=grad[p+2,:]+gradientIntegratedKernel(tend,p+2)\n",
    "        \n",
    "        li = max(index-30,0)\n",
    "        temp1 = t[p][index]-t[p][li:index]\n",
    "        decayFactor = np.sum(exponentialKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "        factor1=  gradientExpKernel(temp1,adjacentKernel[p][0])\n",
    "        jT = MapT1T2[p][t[p][index]]\n",
    "        otherP = (p==0)*1\n",
    "        if(jT != None):\n",
    "            j = np.asscalar(jT[0])\n",
    "            lj = max(j-30,0)\n",
    "            temp2 = t[p][index]-t[otherP][lj:j+1]\n",
    "            decayFactor = decayFactor + np.sum(exponentialKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            factor2 = gradientExpKernel(temp2,adjacentKernel[p][1])\n",
    "        \n",
    "        lam = mu[p]+decayFactor\n",
    "        grad[adjacentKernel[p][0],:] = grad[adjacentKernel[p][0],:] - (1/lam)*factor1\n",
    "        \n",
    "        grad[adjacentKernel[p][1],:] = grad[adjacentKernel[p][1],:] - (1/lam)*factor2\n",
    "        \n",
    "    print(grad/max(iArray.shape))\n",
    "                             \n",
    "    return grad/max(iArray.shape)\n",
    "\n",
    "def sgdHawkes():\n",
    "    global para\n",
    "    lr =0.01\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "    epsilon = 1e-8\n",
    "    count = 0\n",
    "    bestll = 1e8\n",
    "    bestpara = np.zeros([4,2])\n",
    "    m_t = np.zeros([4,2])\n",
    "    v_t = np.zeros([4,2])\n",
    "    lenT1 = len(t[0][:])\n",
    "    lenT2 = len(t[1][:])\n",
    "    totalLength = lenT1+lenT2\n",
    "    tCompressed = np.zeros((2,totalLength)).astype(int)\n",
    "    tCompressed[:,lenT1:totalLength]=1\n",
    "    tCompressed[1,0:lenT1]=range(0,lenT1,1)\n",
    "    tCompressed[1,lenT1:totalLength]=range(0,lenT2,1)\n",
    "    \n",
    "    for epochs in range(1,30,1):\n",
    "        \n",
    "        rsample = np.random.choice(totalLength,totalLength,replace = False)\n",
    "        for i in range(0,len(rsample),100):\n",
    "            count=count+1 \n",
    "            grad = gradientll(tCompressed[:,rsample[i:i+100]])\n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad*grad)\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            para = para-(lr*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            \n",
    "            error=loglikelihood()\n",
    "            error2=alternate_loglikelihood()\n",
    "            bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            para = np.maximum(para,0)\n",
    "            bestll = min(bestll,error)\n",
    "            \n",
    "            print(i,error,error2, bestll,'\\n',para)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara  \n",
    "    \n",
    "\n",
    "## plots\n",
    "def plotKernels(p):\n",
    "    showIntegratedKernel = True\n",
    "    dx=0.01\n",
    "    \n",
    "    tk = np.arange(0,5,dx)\n",
    "    interestX = Dict[p]['inflection']\n",
    "    \n",
    "    y1 = alpha*np.exp(-beta*tk).reshape(-1)\n",
    "    y2 = nnKernel(tk.reshape(1,-1),p).reshape(-1)\n",
    "    y3 = (alpha/beta)*(1 - np.exp(-beta*tk)).reshape(-1)\n",
    "    y4= np.cumsum(y2*dx).reshape(-1)\n",
    "    y5 = nnIntegratedKernel(tk.reshape(1,-1),p).reshape(-1)\n",
    "    \n",
    "    plt.plot(tk.reshape(-1), y1,'tab:blue')\n",
    "    plt.plot(tk.reshape(-1),y2,'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        plt.plot(tk.reshape(-1), y3,'tab:green')\n",
    "        plt.plot(tk.reshape(-1),y4,'tab:red')\n",
    "        plt.plot(tk.reshape(-1),y5,'tab:brown')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.pause(0.005)\n",
    "\n",
    "def plotKernelsAll():\n",
    "    \n",
    "    showIntegratedKernel = False\n",
    "    dx=0.01\n",
    "    \n",
    "    tk = np.arange(0,5,dx)\n",
    "    y2 = np.zeros([len(tk),4])\n",
    "    y4 = np.zeros([len(tk),4])\n",
    "    y5 = np.zeros([len(tk),4])\n",
    "    \n",
    "    for p in range(4):\n",
    "        interestX = Dict[p]['inflection']\n",
    "    \n",
    "        y2[:,p] = nnKernel(tk.reshape(1,-1),p)\n",
    "\n",
    "        y4[:,p] = np.cumsum(y2[:,p]*dx)\n",
    "        y5[:,p] = nnIntegratedKernel(tk.reshape(1,-1),p)\n",
    "\n",
    "\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    p=0\n",
    "    fig.suptitle('Sharing x per column, y per row')\n",
    "    ax1.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax1.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax1.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p=1\n",
    "    \n",
    "    ax2.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax2.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax2.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p= 2\n",
    "    \n",
    "    ax3.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax3.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax3.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "    \n",
    "    p=3\n",
    "    ax4.plot(tk.reshape(-1),y2[:,p],'tab:orange')\n",
    "    #\n",
    "    if(showIntegratedKernel):\n",
    "        ax4.plot(tk.reshape(-1),y4[:,p],'tab:red')\n",
    "        ax4.plot(tk.reshape(-1),y5[:,p],'tab:brown')\n",
    "   \n",
    "    return\n",
    "## Code for Neural Hawkes starts here##\n",
    "\n",
    "def inflectionPoints():\n",
    "    \n",
    "    for p in range(4):\n",
    "        alphas = Alphas[:,p]\n",
    "    \n",
    "        betas = Betas[:,p]\n",
    "        beta0 = Beta0[:,p]\n",
    "    \n",
    "        div = betas+epsilon*(np.abs(betas)<epsilon) # potentially error prone\n",
    "        x = -beta0/div\n",
    "        interestX1 = x*(x>0)\n",
    "        alwaysInclude = (x<=0)*(betas>0) #dont change\n",
    "        alwaysExclude = (x<=0)*(betas<0)\n",
    "        tempX = x*(~alwaysInclude)*(~alwaysExclude)\n",
    "        interestX1 = tempX[interestX1>0]\n",
    "        interestX = np.sort(interestX1)\n",
    "        interestX = np.append(0,interestX)\n",
    "        \n",
    "        Dict[p]['inflection'] = interestX\n",
    "   \n",
    "        con = alphas*betas\n",
    "        Dict[p]['constant'] = con\n",
    "    return\n",
    "\n",
    "def nnIntegratedKernel(x,p):\n",
    "    x = x.reshape(-1)\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p].reshape(-1,1)\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    const1 = Dict[p]['constant']\n",
    "    interestX = Dict[p]['inflection']\n",
    "    precalculate_integrate(p)\n",
    "    \n",
    "    y = np.zeros([1,max(x.shape)])\n",
    "    \n",
    "    for i in range(0,max(x.shape)):\n",
    "        xi = x[i]\n",
    "        if(xi>0):\n",
    "            iP = max(interestX[interestX<xi])\n",
    "            n1 = betas*(xi-epsilon) + beta0\n",
    "            dn1 = (n1>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "            term1 = nnKernel(xi,p)*((const!=0)+xi*(const==0))\n",
    "            term2 = nnKernel(iP,p)*((const!=0)+iP*(const==0))\n",
    "            \n",
    "            const = (const)*(const!=0)+(const==0)*1.0\n",
    "            \n",
    "            prev_term = Dict_integrate[p][iP]\n",
    "            \n",
    "            y[0,i] = prev_term + ((term1-term2)/(const))\n",
    "        \n",
    "       \n",
    "               \n",
    "\n",
    "    return y\n",
    "\n",
    "def nnKernel(x,p):\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p]\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y    \n",
    "\n",
    "\n",
    "def initializeParams(nNeurons):\n",
    "    \n",
    "    \n",
    "    for kernel in range(4):\n",
    "        Alphas[:,kernel] = -(np.random.uniform(0,1,nNeurons)).reshape(-1)*0.5\n",
    "        Alpha0[:,kernel] = -np.random.uniform(0,1,1)*0.5\n",
    "        Betas[:,kernel] = (np.random.uniform(0,1,nNeurons)).reshape(-1)*0.5\n",
    "        Beta0[:,kernel] = np.random.uniform(0,1,nNeurons).reshape(-1)*0.05\n",
    "       \n",
    "    return mu\n",
    "\n",
    "\n",
    "\n",
    "def precalculate_integrate(kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    iP = Dict[kernel]['inflection']\n",
    "    const1 = Dict[kernel]['constant']\n",
    "    Dict_integrate[kernel].clear()\n",
    "    Dict_integrate[kernel][0]=0\n",
    "    y = 0\n",
    "    for index in range(1,len(iP)):\n",
    "        n1 = betas*(iP[index]-epsilon) + beta0\n",
    "        dn1 = (n1>0)\n",
    "        const =np.dot(const1.T,dn1)\n",
    "            \n",
    "            \n",
    "        term1 = nnKernel(iP[index],kernel)*((const!=0)+iP[index]*(const==0))\n",
    "        term2 = nnKernel(iP[index-1],kernel)*((const!=0)+iP[index-1]*(const==0))\n",
    "            \n",
    "        const = (const)*(const!=0)+(const==0)*1.0\n",
    "              \n",
    "        y= y + ((term1-term2)/(const))\n",
    "        Dict_integrate[kernel][iP[index]]=y\n",
    "    return\n",
    "    \n",
    "def precalculate_gradient(kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    iP = Dict[kernel]['inflection']\n",
    "    \n",
    "    const1 = Dict[kernel]['constant']\n",
    "    Dict_gradient[kernel].clear()\n",
    "    Dict_gradient[kernel][0]=list([gradA0,gradA,gradB1,gradB0])\n",
    "    for index in range(1,len(iP)):\n",
    "            \n",
    "            n0pr = betas*(iP[index-1]+epsilon)+beta0\n",
    "            n1pr = betas*(iP[index]-epsilon) + beta0\n",
    "            dn1 = (n1pr>0)\n",
    "            dn0 = (n0pr>0)\n",
    "            const =np.dot(const1.T,dn1)\n",
    "            indicator = const==0\n",
    "            const = const*(const!=0)+1*(const==0)\n",
    "            n0 = betas*(iP[index-1])+beta0\n",
    "            n1 = betas*(iP[index]) + beta0\n",
    "            \n",
    "            fac1 = nnKernel(iP[index],kernel)\n",
    "            fac2 = nnKernel(iP[index-1],kernel)\n",
    "            gradA0 = gradA0 + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(iP[index]-iP[index-1])\n",
    "            gradA = gradA -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            gradB1 = gradB1 -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "            gradB1 = gradB1 + ((1/const)*(alphas)*(fac1*iP[index]*dn1-fac2*iP[index-1]*dn0))*(~indicator)\n",
    "            gradB0 = gradB0+ ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "            Dict_gradient[kernel][iP[index]]= list([gradA0,gradA,gradB1,gradB0])\n",
    "            \n",
    "    return\n",
    "    \n",
    "def gradientNNIntegratedKernel(tend,kernel):\n",
    "    \n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel].reshape(-1,1)\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    \n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    \n",
    "    if(tend>0):\n",
    "        const1 = Dict[kernel]['constant']\n",
    "        interestX = Dict[kernel]['inflection']\n",
    "            \n",
    "        iP = max(interestX[interestX<tend])\n",
    "        n1pr = betas*(tend-epsilon) + beta0\n",
    "        n0pr = betas*(tend+epsilon)+beta0\n",
    "           \n",
    "        dn1 = (n1pr>0)\n",
    "        dn0 = (n0pr>0)\n",
    "        const =np.dot(const1.T,dn1)\n",
    "        indicator = const==0\n",
    "        const = const*(const!=0)+1*(const==0)\n",
    "        n0 = betas*(iP)+beta0\n",
    "        n1 = betas*(tend) + beta0\n",
    "        fac1 = nnKernel(tend,kernel)\n",
    "        fac2 = nnKernel(iP,kernel)\n",
    "        gradients = Dict_gradient[kernel][iP]\n",
    "            \n",
    "            \n",
    "        gradA0 = gradA0+gradients[0] + ((1/const)*(fac1-fac2))*(~indicator)+(fac1)*(indicator)*(tend-iP)\n",
    "            \n",
    "        gradA = gradA + gradients[1] -((1/(const*const))*(betas*dn1)*(fac1-fac2))*(~indicator)\n",
    "        gradA = gradA + ((1/const)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0)))*(~indicator)\n",
    "            \n",
    "        gradB1 = gradB1+gradients[2] -((1/(const*const))*(alphas*dn1)*(fac1-fac2))*(~indicator)\n",
    "        gradB1 = gradB1+ ((1/const)*(alphas)*(fac1*tend*dn1-fac2*iP*dn0))*(~indicator)\n",
    "    \n",
    "        gradB0 =gradB0+ gradients[3] + ((1/const)*((alphas)*(fac1*np.maximum(n1,0)-fac2*np.maximum(n0,0))))*(~indicator)\n",
    "    return list([gradA0,gradA,gradB1,gradB0])\n",
    "\n",
    "def gradientNNKernel(temp,kernel):\n",
    "    alphas = Alphas[:,kernel].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,kernel]\n",
    "    betas = Betas[:,kernel].reshape(-1,1)\n",
    "    beta0 = Beta0[:,kernel].reshape(-1,1)\n",
    "    \n",
    "    gradA = alphas*0\n",
    "    gradB1 = gradA\n",
    "    gradB0 = gradB1\n",
    "    gradA0=0\n",
    "    fac1 = nnKernel(temp.reshape(1,-1),kernel)\n",
    "    \n",
    "    \n",
    "   \n",
    "    n1 = np.dot(betas,temp.reshape(1,-1)) + beta0\n",
    "    gradA = gradA + np.sum(fac1*np.maximum(n1,0),axis=1).reshape(-1,1)\n",
    "        \n",
    "    gradA0 = gradA0 + np.sum(fac1)\n",
    "        \n",
    "    gradB1 = gradB1 + np.sum(fac1*(n1>0)*alphas*temp.reshape(1,-1),axis=1).reshape(-1,1)\n",
    "    gradB0 = gradB0 + np.sum(fac1*(n1>0)*alphas,axis=1).reshape(-1,1)\n",
    "    return list([gradA0,gradA,gradB1,gradB0])\n",
    "    \n",
    "    \n",
    "\n",
    "def gradientNetwork(iArray):\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    gradA0 = np.zeros([1,4])\n",
    "    gradA = np.zeros([nNeurons,4])\n",
    "    gradB1 = np.zeros([nNeurons,4])\n",
    "    gradB0 = np.zeros([nNeurons,4])\n",
    "    grad_mu = np.zeros(2)\n",
    "    nSamples = max(iArray.shape)\n",
    "    \n",
    "    for kernel in range(0,4,1):\n",
    "        precalculate_gradient(kernel)\n",
    "    \n",
    "    for i in range(0,nSamples,1):\n",
    "        p = iArray[0,i]\n",
    "        index = iArray[1,i]\n",
    "        tend = (T-t[p][index])\n",
    "        out = gradientNNIntegratedKernel(tend,p)\n",
    "       \n",
    "        gradA0[:,p]=gradA0[:,p]+out[0]\n",
    "        gradA[:,p]=gradA[:,p]+out[1].reshape(-1)\n",
    "        gradB1[:,p]=gradB1[:,p]+out[2].reshape(-1)\n",
    "        gradB0[:,p]=gradB0[:,p]+out[3].reshape(-1)\n",
    "        \n",
    "        out = gradientNNIntegratedKernel(tend,p+2)\n",
    "        gradA0[:,p+2]=gradA0[:,p+2]+out[0]\n",
    "        gradA[:,p+2]=gradA[:,p+2]+out[1].reshape(-1)\n",
    "        gradB1[:,p+2]=gradB1[:,p+2]+out[2].reshape(-1)\n",
    "        gradB0[:,p+2]=gradB0[:,p+2]+out[3].reshape(-1)\n",
    "        \n",
    "        li = max(index-40,0)\n",
    "        kernel0 = adjacentKernel[p][0]\n",
    "        kernel1 = adjacentKernel[p][1]\n",
    "        temp1 = t[p][index]-t[p][li:index]\n",
    "        decayFactor = np.sum(nnKernel(temp1.reshape(1,-1),kernel0))\n",
    "        out =  gradientNNKernel(temp1,kernel0)\n",
    "        jT = MapT1T2[p][t[p][index]]\n",
    "        otherP = (p==0)*1\n",
    "        if(jT != None):\n",
    "            j = np.asscalar(jT[0][0])\n",
    "            lj = max(j-40,0)\n",
    "            temp2 = t[p][index]-t[otherP][lj:j+1]\n",
    "            decayFactor = decayFactor + np.sum(nnKernel(temp2.reshape(1,-1),kernel1))\n",
    "            out2 = gradientNNKernel(temp2,kernel1)\n",
    "        \n",
    "        lam = mu[p]+decayFactor\n",
    "        invLam = (1/lam)\n",
    "        \n",
    "        \n",
    "        gradA0[:,kernel0]=gradA0[:,kernel0] -invLam*out[0]\n",
    "        gradA[:,kernel0]=gradA[:,kernel0] -invLam*out[1].reshape(-1)\n",
    "        gradB1[:,kernel0]=gradB1[:,kernel0]-invLam*out[2].reshape(-1)\n",
    "        gradB0[:,kernel0]=gradB0[:,kernel0] -invLam*out[3].reshape(-1)\n",
    "        \n",
    "        if(jT != None):\n",
    "            gradA0[:,kernel1]=gradA0[:,kernel1] -invLam*out2[0]\n",
    "            gradA[:,kernel1]=gradA[:,kernel1] -invLam*out2[1].reshape(-1)\n",
    "            gradB1[:,kernel1]=gradB1[:,kernel1]-invLam*out2[2].reshape(-1)\n",
    "            gradB0[:,kernel1]=gradB0[:,kernel1] -invLam*out2[3].reshape(-1)\n",
    "        grad_mu[p]=grad_mu[p]+((t[p][index]-t[p][index-1])-(1/lam))*(index >0) \n",
    "        \n",
    "    gradA0 = gradA0/(nSamples)\n",
    "    gradA = gradA/nSamples\n",
    "    gradB1 = gradB1/nSamples\n",
    "    gradB0 = gradB0/nSamples\n",
    "        \n",
    "    return list([gradA0,gradA,gradB1,gradB0,grad_mu])\n",
    "     \n",
    "\n",
    "\n",
    "def nnLoglikelihood():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(nnIntegratedKernel(tend.reshape(1,-1),p))\n",
    "       \n",
    "        \n",
    "        a = a + np.sum(nnIntegratedKernel(tend.reshape(1,-1),p+2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ll = ll+ mu[p]*T+a\n",
    "       \n",
    "        \n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(nnKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(nnKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            \n",
    "            ll = ll+logLam\n",
    "       \n",
    "    return ll\n",
    "\n",
    "def sgdNeuralHawkesBiVariate(nEpochs,lr):\n",
    "    \n",
    "    global Alpha0\n",
    "    global Alphas\n",
    "    global Betas\n",
    "    global Beta0\n",
    "    global mu\n",
    "    \n",
    "    lr2 =lr*0.1\n",
    "    lr_mu = lr*0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 =0.999\n",
    "  \n",
    "    \n",
    "    bestll = 1e8\n",
    "    neg_ll = []\n",
    "    \n",
    "    optimalParams = list([Alpha0,Alphas,Betas,Beta0])\n",
    "    \n",
    "    m_t_A = np.zeros([nNeurons,4])\n",
    "    m_t_A0 =np.zeros([1,4])\n",
    "    m_t_B= np.zeros([nNeurons,4])\n",
    "    m_t_B0= np.zeros([nNeurons,4])\n",
    "    m_t = 0\n",
    "    v_t_A = np.zeros([nNeurons,4])\n",
    "    v_t_A0 =np.zeros([1,4])\n",
    "    v_t_B= np.zeros([nNeurons,4])\n",
    "    v_t_B0= np.zeros([nNeurons,4])\n",
    "    v_t = 0\n",
    "    count = 0\n",
    "    lenT1 = len(t[0][:])\n",
    "    lenT2 = len(t[1][:])\n",
    "    totalLength = lenT1+lenT2\n",
    "    tCompressed = np.zeros((2,totalLength)).astype(int)\n",
    "    tCompressed[:,lenT1:totalLength]=1\n",
    "    tCompressed[1,0:lenT1]=range(0,lenT1,1)\n",
    "    tCompressed[1,lenT1:totalLength]=range(0,lenT2,1)\n",
    "    \n",
    "    for epochs in range(1,30,1):\n",
    "        inflectionPoints()\n",
    "        rsample = np.random.choice(totalLength,totalLength,replace = False)\n",
    "        for i in range(0,len(rsample),20):\n",
    "            count=count+1 \n",
    "            grad = gradientNetwork(tCompressed[:,rsample[i:i+20]])\n",
    "            \n",
    "            m_t = beta_1*m_t + (1-beta_1)*grad[4]\t#updates the moving averages of the gradient\n",
    "            v_t = beta_2*v_t + (1-beta_2)*(grad[4]*grad[4])\t#updates the moving averages of the squared gradient\n",
    "            m_cap = m_t/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap = v_t/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            mu = mu-(lr_mu*m_cap)/(np.sqrt(v_cap)+epsilon)\n",
    "            mu = np.maximum(mu,0)\n",
    "           \n",
    "           \n",
    "            m_t_A0 = beta_1*m_t_A0 + (1-beta_1)*grad[0]\t#updates the moving averages of the gradient\n",
    "            v_t_A0 = beta_2*v_t_A0 + (1-beta_2)*(grad[0]*grad[0])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A0 = m_t_A0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A0 = v_t_A0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Alpha0 = Alpha0-(lr*m_cap_A0)/(np.sqrt(v_cap_A0)+epsilon) \n",
    "                         \n",
    "                   \n",
    "            m_t_A = beta_1*m_t_A + (1-beta_1)*grad[1]\t#updates the moving averages of the gradient\n",
    "            v_t_A = beta_2*v_t_A + (1-beta_2)*(grad[1]*grad[1])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_A = m_t_A/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_A = v_t_A/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Alphas = Alphas-(lr*m_cap_A)/(np.sqrt(v_cap_A)+epsilon)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "                \n",
    "            m_t_B = beta_1*m_t_B + (1-beta_1)*grad[2]\t#updates the moving averages of the gradient\n",
    "            v_t_B = beta_2*v_t_B + (1-beta_2)*(grad[2]*grad[2])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B = m_t_B/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B= v_t_B/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Betas = Betas-(lr2*m_cap_B)/(np.sqrt(v_cap_B)+epsilon)\n",
    "            \n",
    "            \n",
    "            m_t_B0 = beta_1*m_t_B0 + (1-beta_1)*grad[3]\t#updates the moving averages of the gradient\n",
    "            v_t_B0 = beta_2*v_t_B0 + (1-beta_2)*(grad[3]*grad[3])\t#updates the moving averages of the squared gradient\n",
    "            m_cap_B0 = m_t_B0/(1-(beta_1**count))\t\t#calculates the bias-corrected estimates\n",
    "            v_cap_B0 = v_t_B0/(1-(beta_2**count))\t\t#calculates the bias-corrected estimates\n",
    "            Beta0 = Beta0 -(lr2*m_cap_B0)/(np.sqrt(v_cap_B0)+epsilon)\n",
    "          \n",
    "            inflectionPoints()\n",
    "            \n",
    "        error=nnLoglikelihood()\n",
    "        neg_ll.append(error)\n",
    "            #bestpara = para*(bestll>=error)+bestpara*(bestll<error)\n",
    "            \n",
    "        if(bestll > error):\n",
    "            optimalParams = list([Alpha0,Alphas,Betas,Beta0,mu])\n",
    "                \n",
    "               \n",
    "            \n",
    "        bestll = min(bestll,error)\n",
    "        print(i,epochs,bestll,error,mu)   #iteration, -loglikelihood, bestloglik, currentpara, bestpara\n",
    "    return optimalParams,neg_ll,nEpochs\n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 83574\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/lekhapriya/SNH/master/Combined.csv\"\n",
    "dataset = pd.read_csv(url, names=['Timestamp', 'Price','Volume','Buyer ID','Seller ID','Buyer is market maker'])\n",
    "dataset = dataset.sort_values(dataset.columns[0])\n",
    "buy_df = dataset.loc[~dataset.iloc[:,5]]\n",
    "sell_df = dataset.loc[dataset.iloc[:,5]]\n",
    "buy = buy_df.drop_duplicates(['Buyer ID'])\n",
    "sell = sell_df.drop_duplicates(['Seller ID'])\n",
    "\n",
    "start_time = (dataset.iloc[0,0])/1000\n",
    "\n",
    "#ms to seconds\n",
    "utc_sell = (sell.iloc[:,0].values)/1000\n",
    "utc_buy = (buy.iloc[:,0].values)/1000\n",
    "\n",
    "#convert utc to seconds since t0\n",
    "t_sell = []\n",
    "for i in range(0,len(utc_sell)) :\n",
    "    t_sell.append(utc_sell[i]-start_time)\n",
    "    \n",
    "t_buy = []\n",
    "for i in range(0,len(utc_buy)) :\n",
    "    t_buy.append(utc_buy[i]-start_time)\n",
    "    \n",
    "t = list([np.array(t_buy),np.array(t_sell)])\n",
    "\n",
    "print('N =',len(t_sell)+len(t_buy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:531: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:595: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83560 1 -34541.65756594704 -34541.65756594704 [0.73956881 0.93704958]\n",
      "83560 2 -34908.01929111942 -34908.01929111942 [0.78268832 1.11170518]\n",
      "83560 3 -34917.54090373375 -34917.54090373375 [0.82893257 1.12885674]\n",
      "83560 4 -35268.1705598983 -35268.1705598983 [0.8480353  1.11630913]\n",
      "83560 5 -35268.17056933371 -35268.17056933371 [0.86687935 1.11116911]\n",
      "83560 6 -35500.12968848445 -35500.12968848445 [0.83618981 1.135563  ]\n",
      "83560 7 -35744.47964074473 -35744.47964074473 [0.83048124 1.1080742 ]\n",
      "83560 8 -35764.75665259561 -35764.75665259561 [0.83787047 1.10810223]\n",
      "83560 9 -35810.979100375545 -35810.979100375545 [0.84121881 1.08504216]\n",
      "83560 10 -35923.77628818611 -35923.77628818611 [0.80623258 1.10915654]\n",
      "83560 11 -35965.65153452888 -35965.65153452888 [0.83752309 1.11403564]\n",
      "83560 12 -35965.65153452888 -35960.89551548279 [0.80177023 1.14670045]\n",
      "83560 13 -36057.24439787207 -36057.24439787207 [0.84635812 1.15758702]\n",
      "83560 14 -36509.07923268542 -36509.07923268542 [0.87161684 1.14713755]\n",
      "83560 15 -36930.669185839324 -36930.669185839324 [0.85440995 1.17071262]\n",
      "83560 16 -37104.89303392488 -37104.89303392488 [0.84738755 1.18576159]\n",
      "83560 17 -37479.88750324241 -37479.88750324241 [0.83454284 1.1360216 ]\n",
      "83560 18 -37719.56314990462 -37719.56314990462 [0.88369081 1.17219666]\n",
      "83560 19 -37935.0833768 -37935.0833768 [0.85843082 1.14813887]\n",
      "83560 20 -38065.90385994533 -38065.90385994533 [0.86244664 1.12659647]\n",
      "83560 21 -38237.52136576927 -38237.52136576927 [0.87338038 1.13404031]\n",
      "83560 22 -38627.04076551856 -38627.04076551856 [0.87909982 1.15146096]\n",
      "83560 23 -39124.64717405927 -39124.64717405927 [0.87044017 1.16517672]\n",
      "83560 24 -39426.89369587386 -39426.89369587386 [0.85128097 1.14870663]\n",
      "83560 25 -39711.02429336515 -39711.02429336515 [0.87019159 1.18747631]\n",
      "83560 26 -39804.2743946429 -39804.2743946429 [0.83818925 1.20401156]\n",
      "83560 27 -40099.17842006188 -40099.17842006188 [0.87600789 1.15083775]\n",
      "83560 28 -40258.075879614335 -40258.075879614335 [0.87269292 1.16621241]\n",
      "83560 29 -40372.31398401331 -40372.31398401331 [0.85638244 1.18046618]\n",
      "--- 5259.9098064899445 seconds ---\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array(t[0][:]).reshape(-1)\n",
    "t2 = np.array(t[1][:]).reshape(-1)\n",
    "MapT1T2[0] = createMapAtoBIndex(t1,t2)\n",
    "MapT1T2[1] = createMapAtoBIndex(t2,t1)\n",
    "\n",
    "\n",
    "initializeParams(nNeurons)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "SGD = sgdNeuralHawkesBiVariate(30,0.01)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotKernelsAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnOptimalKernel(x,p):\n",
    "    optimalkernel = SGD[0]\n",
    "    Alpha0 = optimalkernel[0]\n",
    "    Alphas = optimalkernel[1]\n",
    "    Betas = optimalkernel[2]\n",
    "    Beta0 = optimalkernel[3]\n",
    "    alphas = Alphas[:,p].reshape(-1,1)\n",
    "    alpha0 = Alpha0[:,p]\n",
    "    betas = Betas[:,p].reshape(-1,1)\n",
    "    beta0 = Beta0[:,p].reshape(-1,1)\n",
    "    n1 = np.maximum(np.dot(betas,x) + beta0,0.)\n",
    "    y = np.dot(alphas.T,n1) + alpha0\n",
    "    \n",
    "    y = np.exp(y)\n",
    "    return y\n",
    "\n",
    "def customIntegrateKernel(tend,p):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = nnOptimalKernel(t3.reshape(1,-1),p).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def SNH_Neg_LL():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernel(tend.reshape(-1),p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernel(tend.reshape(-1),p+2))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(nnOptimalKernel(temp1.reshape(1,-1),adjacentKernel[p][0]))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(nnOptimalKernel(temp2.reshape(1,-1),adjacentKernel[p][1]))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:55: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40355.50520479907\n"
     ]
    }
   ],
   "source": [
    "print(SNH_Neg_LL())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_SNH = [0.85638244 1.18046618] mu_EM = [0.47840954 0.67060714] mu_WH = [0.09314908 0.2692201 ]\n"
     ]
    }
   ],
   "source": [
    "#Kernel plot\n",
    "from tick.hawkes import HawkesEM, HawkesConditionalLaw,HawkesSumExpKern\n",
    "timestamp = t\n",
    "\n",
    "em = HawkesEM(kernel_support = 6, kernel_size=100, n_threads=1, verbose=False, tol=1e-3)\n",
    "wh = HawkesConditionalLaw(n_quad=200)\n",
    "\n",
    "#fit the model\n",
    "em.fit(t)\n",
    "wh.fit(t)\n",
    "\n",
    "                          \n",
    "optimalParams = SGD[0]\n",
    "print('mu_SNH =',optimalParams[4],'mu_EM =', em.baseline,'mu_WH =',wh.baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherKernel={0:[1],1:[0]}\n",
    "\n",
    "def customIntegrateKernelEM(tend,m,n):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = em.get_kernel_values(m,n,t3.reshape(1,-1)).reshape(-1)\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def EM_Neg_ll():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernelEM(tend.reshape(-1),p,p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernelEM(tend.reshape(-1),otherKernel[p][0],p))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            decayFactor = np.sum(em.get_kernel_values(p,p, temp1.reshape(1,-1)))\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                decayFactor = decayFactor + np.sum(em.get_kernel_values(p,otherKernel[p],temp2.reshape(1,-1)))\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll\n",
    "\n",
    "#to calculate Neg_ll of WH\n",
    "\n",
    "def customIntegrateKernelWH(tend,m,n):\n",
    "    dx=0.001\n",
    "    tk = np.arange(0,5,dx)\n",
    "    t2 = np.unique(np.concatenate([tend, tk]))\n",
    "    t3 = np.sort(t2)\n",
    "    dx = np.diff(t3)\n",
    "    \n",
    "    ytemp = wh.get_kernel_values(m,n,t3.reshape(1,-1)).reshape(-1)\n",
    "    ytemp[ytemp < 0] = 0\n",
    "    yTrap= 0.5*(ytemp[0:-1]+ytemp[1:])\n",
    "    \n",
    "    a = np.multiply(yTrap,dx)\n",
    "    a1 = np.cumsum(np.concatenate([np.array([0]), a]).reshape(-1))\n",
    "    b = a1[[np.where(t3 ==x)[0][0] for x in tend]]\n",
    "    return b\n",
    "\n",
    "def WH_Neg_ll():\n",
    "    \n",
    "    T = max(t[0][-1],t[1][-1])\n",
    "    ll=0\n",
    "    for p in range(0,2,1):\n",
    "        tend = (T-t[p][:])\n",
    "        a = np.sum(customIntegrateKernelWH(tend.reshape(-1),p,p))\n",
    "    \n",
    "        a = a + np.sum(customIntegrateKernelWH(tend.reshape(-1),otherKernel[p][0],p))\n",
    "        ll = ll+ mu[p]*T+a\n",
    "        ll = ll-np.log(mu[p])\n",
    "        llinit = ll\n",
    "        tp = t[p][:]  \n",
    "        otherP = (p==0)*1\n",
    "        \n",
    "        tOtherP = t[otherP][:]\n",
    "        for i in range(1,len(tp),1):\n",
    "            li = max(i-40,0)\n",
    "            temp1 = tp[i]-tp[li:i]\n",
    "            nk = wh.get_kernel_values(p,p, temp1.reshape(1,-1))\n",
    "            nk[nk < 0] = 0\n",
    "            decayFactor = np.sum(nk)\n",
    "            jT = MapT1T2[p][tp[i]]\n",
    "           \n",
    "            if(jT != None):\n",
    "                j = np.asscalar(jT[0][0])\n",
    "                lj = max(j-40,0)\n",
    "                temp2 = tp[i]-tOtherP[lj:j+1]\n",
    "                nk = wh.get_kernel_values(p,otherKernel[p][0],temp2.reshape(1,-1))\n",
    "                nk[nk < 0] = 0\n",
    "                decayFactor = decayFactor + np.sum(nk)\n",
    "            logLam = -np.log(mu[p]+decayFactor)\n",
    "            ll = ll+logLam\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32780.71838416736\n"
     ]
    }
   ],
   "source": [
    "print(EM_Neg_ll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:92: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3963.6802092938938\n"
     ]
    }
   ],
   "source": [
    "print(WH_Neg_ll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get node\n",
    "c_nodes = em.n_nodes\n",
    "\n",
    "#tk\n",
    "dx=0.01\n",
    "tk = np.arange(0,5,dx)\n",
    "y2 = np.zeros([len(tk),4])\n",
    "\n",
    "\n",
    "for p in range(4):\n",
    "    y2[:,p] = nnOptimalKernel(tk.reshape(1,-1),p)\n",
    "    \n",
    "#plot\n",
    "fig, ax_list_list = plt.subplots(c_nodes,c_nodes, sharex=True,\n",
    "                                         sharey=True,figsize=(8, 8))\n",
    "    \n",
    "p = 0\n",
    "\n",
    "for i, ax_list in enumerate(ax_list_list):\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        y_values = em.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'b--', label=\"EM\")\n",
    "        y_values = wh.get_kernel_values(i, j, tk)\n",
    "        ax.plot(tk, y_values,'g--', label=\"WH\")\n",
    "        y_values = y2[:,p]\n",
    "        ax.plot(tk, y_values,'r--', label=\"SNH\")\n",
    "        p = p+1\n",
    "        \n",
    "        # set x_label for last line\n",
    "        if i == c_nodes - 1:\n",
    "            ax.set_xlabel(r\"$t$\", fontsize=18)\n",
    "\n",
    "        ax.set_ylabel(r\"$\\phi^{%g,%g}(t)$\" % (i, j), fontsize=15)\n",
    "        \n",
    "        legend = ax.legend()\n",
    "        for label in legend.get_texts():\n",
    "            label.set_fontsize(9)\n",
    "plt.axis([-0.1,2.5, -0.2, 4])\n",
    "\n",
    "plt.savefig(\"2D_binance_kernel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
